<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MQTT - 控制报文详情]]></title>
    <url>%2F2019%2F06%2F28%2F2019%2Fmqtt-%E6%8E%A7%E5%88%B6%E6%8A%A5%E6%96%87%E8%AF%A6%E6%83%85%2F</url>
    <content type="text"><![CDATA[通过我的上一篇博客 MQTT - 协议基本详解 可以基本了解到 MQTT 的概念，协议的组成部分（主要由 固定报头、可变报头 和 有效载荷组成），也知道了 MQTT 协议主要支持的 16 种协议，针对不同的协议，客户端/服务端都需要做出不同的行为，本片博客我们会详细描述一下具体某个协议的组成以及客户端/服务端的处理方式，当然，限于篇幅，我们只列举以下几个协议：CONNECT、CONNACK、PUBLICH、PUBACK，这里几个协议分别描述了 MQTT 的建立连接过程、发送消息过程，如果了解更多协议的内容，请参考 MQTT 3.1.1 - 官方手册。 ¶建立连接 MQTT 本身是基于 TCP/IP 协议基础之上协议，依赖于 TCP 连接通道，所有在 TCP 建立握手，建立连接之后，MQTT 协议本身还需要通过 CONNECT 协议报文来确定双方的通信通道，也可以说，建立连接之后的第一个报文必须是 CONNECT 报文。 注意：MQTT 协议规定，CONNECT 报文只能发送多次，如果服务器在接收到 CONNECT 之后重复收到，服务端 必须 认为客户端是不合法的操作，必须 主动断开与客户端的连接。 ¶CONNECT CONNECT 协议报文使用到了 固定报头、可变报头 和 有效载荷 三部分，其中 可变报头 部分包含了 协议名、协议版本、标志位 和 KeepAlive 等值；有效载荷部分包含了 客户端标识符、遗嘱主题、遗嘱消息、用户名、用户密码。 ¶固定报头 固定报头首字节固定为：0x10，剩余长度字节依情况而定，组成结构查看下表： ¶可变报头 可变报头包含以下几个部分：协议名、协议版本、标志位 和 KeepAlive，组成结构查看下表。 ¶协议名 协议名固定为 MQTT，所有该部分字节序固定位：0x00 0x04 0x4d 0x51 0x54 0x54，具体查看以下可变报头图表。 ¶协议版本 客户端用 8 位的无符号值表示协议的修订版本。对于 3.1.1 版协议，协议级别字段的值是 4(0x04)。如果发现不支持的协议级别，服务端必须给发送一个返回码为 0x01（不支持的协议级别）的 CONNACK 报文响应 CONNECT 报文，然后断开客户端的连接。 ¶标志位 bit 7 6 5 4 3 2 1 0 dec User Name Flag Password Flag Will Retain Will QoS Will Flag Clean Session Reserved val 1 1 1 1 1 1 1 1 标志位用于描述某个值的存在与否，使用 1 和 0 来标记，比如：User Name Flag 如果设置为 1，则表示 有效载荷 - 用户名 部分数据存在。 清理会话 Clean Session 位置：连接标志字节的第1位 这个二进制位指定了会话状态的处理方式。 客户端和服务端可以保存会话状态，以支持跨网络连接的可靠消息传输。这个标志位用于控制会话状态的生存时间。 如果清理会话（CleanSession）标志被设置为0，服务端 必须 基于当前会话（使用客户端标识符识别）的状态恢复与客户端的通信。如果没有与这个客户端标识符关联的会话，服务端必须创建一个新的会话。在连接断开之后，当连接断开后，客户端和服务端 必须保存会话信息。当清理会话标志为0的会话连接断开之后，服务端 必须 将之后的QoS 1和QoS 2级别的消息保存为会话状态的一部分，如果这些消息匹配断开连接时客户端的任何订阅 [MQTT-3.1.2-5]。服务端也 可以 保存满足相同条件的QoS 0级别的消息。 如果清理会话（CleanSession）标志被设置为1，客户端和服务端必须丢弃之前的任何会话并开始一个新的会话。会话仅持续和网络连接同样长的时间。与这个会话关联的状态数据不能被任何之后的会话重用。 客户端的会话状态包括： 已经发送给服务端，但是还没有完成确认的QoS 1和QoS 2级别的消息 已从服务端接收，但是还没有完成确认的QoS 2级别的消息。 服务端的会话状态包括： 会话是否存在，即使会话状态的其它部分都是空。 客户端的订阅信息。 已经发送给客户端，但是还没有完成确认的 QoS 1 和 QoS 2 级别的消息。 即将传输给客户端的 QoS 1 和 QoS 2 级别的消息。 已从客户端接收，但是还没有完成确认的 QoS 2 级别的消息。 可选，准备发送给客户端的 QoS 0 级别的消息。 保留消息不是服务端会话状态的一部分，会话终止时不能删除保留消息。 当清理会话标志被设置为1时，客户端和服务端的状态删除不需要是原子操作。 非规范评注 为了确保在发生故障时状态的一致性，客户端应该使用会话状态标志1重复请求连接，直到连接成功。 非规范评注 一般来说，客户端连接时总是将清理会话标志设置为0或1，并且不交替使用两种值。这个选择取决于具体的应用。清理会话标志设置为1的客户端不会收到旧的应用消息，而且在每次连接成功后都需要重新订阅任何相关的主题。清理会话标志设置为0的客户端会收到所有在它连接断开期间发布的QoS 1和QoS 2级别的消息。因此，要确保不丢失连接断开期间的消息，需要使用QoS 1或 QoS 2级别，同时将清理会话标志设置为0。 非规范评注 清理会话标志 0 的客户端连接时，它请求服务端在连接断开后保留它的 MQTT 会话状态。如果打算在之后的某个时间点重连到这个服务端，客户端连接应该只使用清理会话标志 0。当客户端决定之后不再使用这个会话时，应该将清理会话标志设置为0最后再连接一次，然后断开连接。 遗嘱标志 Will Flag 位置：连接标志的第2位。 遗嘱标志（Will Flag）被设置为1，表示如果连接请求被接受了，遗嘱（Will Message）消息 必须 被存储在服务端并且与这个网络连接关联。之后网络连接关闭时，服务端 必须 发布这个遗嘱消息，除非服务端收到 DISCONNECT 报文时删除了这个遗嘱消息。 遗嘱消息发布的条件，包括但不限于： 服务端检测到了一个 I/O 错误或者网络故障。 客户端在保持连接（Keep Alive）的时间内未能通讯。 客户端没有先发送 DISCONNECT 报文直接关闭了网络连接。 由于协议错误服务端关闭了网络连接。 如果遗嘱标志被设置为 1，连接标志中的 Will QoS 和 Will Retain 字段会被服务端用到，同时有效载荷中 必须 包含 Will Topi c和 Will Message 字段。 一旦被发布或者服务端收到了客户端发送的DISCONNECT报文，遗嘱消息就必须从存储的会话状态中移除。 如果遗嘱标志被设置为 0，连接标志中的Will QoS和Will Retain字段 必须 设置为 0，并且有效载荷中 不能 包含 Will Topic 和 Will Message 字段。 如果遗嘱标志被设置为0，网络连接断开时，不能发送遗嘱消息。 服务端应该迅速发布遗嘱消息。在关机或故障的情况下，服务端可以推迟遗嘱消息的发布直到之后的重启。如果发生了这种情况，在服务器故障和遗嘱消息被发布之间可能会有一个延迟。 遗嘱 QoS Will QoS 位置：连接标志的第 4 和第 3 位。 这两位用于指定发布遗嘱消息时使用的服务质量等级。 如果遗嘱标志被设置为 0，遗嘱 QoS 也 必须 设置为 0(0x00)。 如果遗嘱标志被设置为 1，遗嘱 QoS 的值可以等于 0(0x00)，1(0x01)，2(0x02)。它的值 不能 等于 3。 遗嘱保留 Will Retain 位置：连接标志的第 5 位。 如果遗嘱消息被发布时需要保留，需要指定这一位的值。 如果遗嘱标志被设置为 0，遗嘱保留（Will Retain）标志也 必须 设置为 0 。 如果遗嘱标志被设置为 1： 如果遗嘱保留被设置为0，服务端必须将遗嘱消息当作非保留消息发布。 如果遗嘱保留被设置为1，服务端必须将遗嘱消息当作保留消息发布。 密码标志 Password Flag 位置：连接标志的第 6 位。 如果密码（Password）标志被设置为0，有效载荷中 不能 包含密码字段。 如果密码（Password）标志被设置为1，有效载荷中 必须 包含密码字段。 如果用户名标志被设置为0，密码标志也必须设置为 0。 用户名标志 User Name Flag 位置：连接标志的第 7 位。 如果用户名（User Name）标志被设置为0，有效载荷中不能包含用户名字段。 如果用户名（User Name）标志被设置为1，有效载荷中必须包含用户名字段。 ¶KeepAlive MQTT 客户端/服务端 必须 实现连接保持机制，该机制表示：它是指在客户端传输完成一个控制报文的时刻到发送下一个报文的时刻。也就是说 MQTT 协议中两个连续报文的时间间隔不能超过 KeepAlive 值，如果超过则客户端/服务端 必须 认为对端已断开，然后关闭该连接。 注：MQTT 协议实现了 PINGREQ 和 PINGRESP 两个协议来完成连接保持机制。 ¶可变报头图表描述 ¶有效载荷 有效载荷部分包含了 客户端标识符、遗嘱主题、遗嘱消息、用户名、用户密码。具体组成结构都为：2 个字节长度 + 具体内容，这样我们就很好解析了，先获取字节长度 len，在读 len 个字节表示该部分数据。 ¶客户端标识符 每个连接都必须有一个唯一的客户端标识符来标识该链接。 他必须满足以下条件： 客户端标识符 (ClientId) 必须 存在而且 必须 是 CONNECT 报文有效载荷的第一个字段。 客户端标识符 必须 是1.5.3节定义的UTF-8编码字符串。 服务端 必须 允许 1 到 23 个字节长的UTF-8编码的客户端标识符，客户端标识符只能包含这些字符：/^[0-91-zA-Z]{1,23}$/（大写字母，小写字母和数字）。 服务端 可以 允许编码后超过23个字节的客户端标识符 (ClientId)。 服务端 可以 允许包含不是上面列表字符的客户端标识符 (ClientId)。 服务端 可以 允许客户端提供一个零字节的客户端标识符 (ClientId) ，如果这样做了，服务端 必须 将这看作特殊情况并分配唯一的客户端标识符给那个客户端。然后它 必须假设客户端提供了那个唯一的客户端标识符，正常处理这个CONNECT报文。 如果客户端提供的 ClientId 为零字节且清理会话标志为 0，服务端必须发送返回码为 0x02（表示标识符不合格）的 CONNACK 报文响应客户端的 CONNECT 报文，然后关闭网络连接。 如果服务端拒绝了这个 ClientId，它 必须 发送返回码为 0x02（表示标识符不合格）的 CONNACK 报文响应客户端的 CONNECT 报文，然后关闭网络连接。 ¶遗嘱主题 如果 遗嘱标志 设置为 1，则该部分就存在。 ¶遗嘱消息 如果 遗嘱标志 设置为 1，则该部分就存在。 如果遗嘱标志被设置为1，有效载荷的下一个字段是遗嘱消息。遗嘱消息定义了将被发布到遗嘱主题的应用消息，这个字段由一个两字节的长度和遗嘱消息的有效载荷组成，表示为零字节或多个字节序列。长度给出了跟在后面的数据的字节数，不包含长度字段本身占用的两个字节。 遗嘱消息被发布到遗嘱主题时，它的有效载荷只包含这个字段的数据部分，不包含开头的两个长度字节。 ¶用户名 如果用户名（User Name）标志被设置为 1，有效载荷的下一个字段就是它。用户名 必须 是 UTF-8编码字符串。服务端可以将它用于身份验证和授权。 ¶用户密码 如果密码（Password）标志被设置为 1，有效载荷的下一个字段就是它。密码字段包含一个两字节的长度字段，长度表示二进制数据的字节数（不包含长度字段本身占用的两个字节），后面跟着 0 到 65535 字节的二进制数据。 ¶有效载荷图表描述 ¶CONNACK CONNACK（Acknowledge Connection Request）是对 CONNECT 报文的响应协议，服务端发送 CONNACK 报文响应从客户端收到的 CONNECT 报文。服务端发送给客户端的第一个报文必须是 CONNACK。 如果客户端在合理的时间内没有收到服务端的 CONNACK 报文，客户端应该关闭网络连接。合理 的时间取决于应用的类型和通信基础设施。 ¶连接确认标志 可变报头第一个字节，用来表示连接确认标志（只使用第一个比特位，其余七位比特保留）。 如果服务端收到清理会话（CleanSession）标志为 1 的连接，除了将 CONNACK 报文中的返回码设置为 0 之外，还 必须 将 CONNACK 报文中的当前会话设置（Session Present）标志为 0。 如果服务端收到一个 CleanSession 为 0 的连接，当前会话标志的值取决于服务端是否已经保存了 ClientId 对应客户端的会话状态。如果服务端已经保存了会话状态，它 必须 将CONNACK报文中的当前会话标志设置为 1。如果服务端没有已保存的会话状态，它 必须 将CONNACK报文中的当前会话设置为0。还需要将CONNACK报文中的返回码设置为 0。 当前会话标志使服务端和客户端在是否有已存储的会话状态上保持一致。 一旦完成了会话的初始化设置，已经保存会话状态的客户端将期望服务端维持它存储的会话状态。如果客户端从服务端收到的当前的值与预期的不同，客户端可以选择继续这个会话或者断开连接。客户端可以丢弃客户端和服务端之间的会话状态，方法是，断开连接，将清理会话标志设置为 1，再次连接，然后再次断开连接。 如果服务端发送了一个包含非零返回码的 CONNACK 报文，它 必须 将当前会话标志设置为 0 。 ¶响应状态码 可变报头最后一个字节，来标识响应状态码，状态码含义如下表： 响应状态码 返回码响应 描述 0x00 连接已接受 连接已被服务端接受 0x01 连接已拒绝，不支持的协议版本 服务端不支持客户端请求的MQTT协议级别 0x02 连接已拒绝，不合格的客户端标识符 客户端标识符是正确的UTF-8编码，但服务端不允许使用 0x03 连接已拒绝，服务端不可用 网络连接已建立，但MQTT服务不可用 0x04 连接已拒绝，无效的用户名或密码 用户名或密码的数据格式无效 0x05 连接已拒绝，未授权 客户端未被授权连接到此服务器 0x06-0xFF 保留 保留 如果认为上表中的所有连接返回码都不太合适，那么服务端 必须 关闭网络连接，不需要发送 CONNACK 报文。 注：CONNACK 没有有效载荷。 ¶发送消息 客户端之间，客户端与服务端之间需要正常的发送消息，我们使用 PUBLISH 来运输数据。 ¶PUBLISH PUBLISH 控制报文是指从客户端向服务端或者服务端向客户端传输一个应用消息。 ¶固定报头 固定报头的首字节 标志位 包含：重发标志 DUP、服务质量等级 QoS Level 和 保留标志 RETAIN。 ¶重发标志 DUP 位置：第 1 个字节，第 3 位 如果 DUP 标志被设置为 0，表示这是客户端或服务端第一次请求发送这个 PUBLISH 报文。如果 DUP 标志被设置为1，表示这可能是一个早前报文请求的重发。 客户端或服务端请求重发一个 PUBLISH 报文时，必须 将 DUP 标志设置为 1。对于 QoS 0 的消息，DUP 标志 必须 设置为 0。 服务端发送 PUBLISH 报文给订阅者时，收到（入站）的 PUBLISH 报文的 DUP 标志的值不会被传播。发送（出站）的 PUBLISH 报文与收到（入站）的 PUBLISH 报文中的 DUP 标志是独立设置的，它的值 必须 单独的根据发送（出站）的 PUBLISH 报文是否是一个重发来确定。 非规范评注 接收者收到一个 DUP 标志为 1 的控制报文时，不能假设它看到了一个这个报文之前的一个副本。 非规范评注 需要特别指出的是，DUP 标志关注的是控制报文本身，与它包含的应用消息无关。当使用 QoS 1 时，客户端可能会收到一个 DUP 标志为 0 的 PUBLISH 报文，这个报文包含一个它之前收到过的应用消息的副本，但是用的是不同的报文标识符。 ¶服务质量等级 QoS Level 位置：第 1 个字节，第 2-1 位。 这个字段表示应用消息分发的服务质量等级保证，服务质量定义如下表： QoS 值 Bit 2 Bit 1 描述 0 0 0 最多分发一次 1 0 1 至少分发一次 2 1 0 只分发一次 - 1 1 保留位 PUBLISH报文 不能 将 QoS 所有的位设置为1。如果服务端或客户端收到 QoS 所有位都为1的 PUBLISH 报文，它 必须 关闭网络连接。 ¶保留标志 RETAIN 位置：第 1 个字节，第 3 位 ¶可变报头 可变报头按顺序包含 主题名 和 报文标识符。 ¶主题名 主题名（Topic Name）用于识别有效载荷数据应该被发布到哪一个信息通道。 主题名 必须 是 PUBLISH 报文可变报头的第一个字段。它 必须 是 UTF-8 编码的字符串。 PUBLISH报文中的主题名 不能 包含通配符。 服务端发送给订阅客户端的 PUBLISH 报文的主题名 必须 匹配该订阅的主题过滤器。 ¶报文标识符 只有当 QoS 等级是 1 或 2 时，报文标识符（Packet Identifier）字段才能出现在 PUBLISH 报文中，用于维护客户端/服务端维护某个消息，如果客户端/服务端接收到该报文并会响应对应的报文（QoS 1 响应 PUBACK，QoS 2 响应 PUBREC），对应报文中会携带该 报文标识符。 ¶有效载荷 有效载荷包含将被发布的应用消息。数据的内容和格式是应用特定的。有效载荷的长度这样计算：用固定报头中的剩余长度字段的值减去可变报头的长度。包含零长度有效载荷的 PUBLISH 报文是合法的。 ¶响应 PUBLISH 报文的接收者 必须 按照根据 PUBLISH 报文中的 QoS 等级发送响应，见下面表格的描述： 服务质量等级 预期响应 QoS 0 无响应 QoS 1 PUBACK报文 QoS 2 PUBREC报文 ¶动作 Actions 客户端使用 PUBLISH 报文发送应用消息给服务端，目的是分发到其它订阅匹配的客户端。 服务端使用 PUBLISH 报文发送应用消息给每一个订阅匹配的客户端。 客户端使用带通配符的主题过滤器请求订阅时，客户端的订阅可能会重复，因此发布的消息可能会匹配多个过滤器。对于这种情况，服务端 必须 将消息分发给所有订阅匹配的 QoS 等级最高的客户端。服务端之后可以按照订阅的 QoS 等级，分发消息的副本给每一个匹配的订阅者。 收到一个 PUBLISH 报文时，接收者的动作取决于 QoS 等级。 如果服务端实现不授权某个客户端发布 PUBLISH 报文，它没有办法通知那个客户端。它 必须 按照正常的 QoS 规则发送一个正面的确认，或者关闭网络连接。 ¶PUBACK PUBACK 是对 QoS 1 的 PUBLISH 报文的响应协议报文。 该协议报文 可变报头 只包含 2 个字节（标识需要回复的 PUBLISH 报文的 标识符 Client Identifier），客户端/服务端收到该报文时，需要将对应的 PUBLISH 报文标记为已送达。 注：CONNACK 没有有效载荷。 ¶其余协议 剩余协议的解释，组成结构请参考 MQTT 3.1.1 - 官方手册。 ¶参考链接 MQTT Version 3.1.1 Official Manual]]></content>
      <categories>
        <category>MQTT</category>
      </categories>
      <tags>
        <tag>MQTT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MQTT - 协议基本详解]]></title>
    <url>%2F2019%2F06%2F27%2F2019%2Fmqtt-%E5%8D%8F%E8%AE%AE%E5%9F%BA%E6%9C%AC%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[MQTT 全称为 Message Queuing Telemetry Transport（消息队列遥测传输）是一种基于发布/订阅范式的 轻量级 消息协议，由 IBM 发布。 MQTT 可以被解释为一种低开销，低带宽占用的即时通讯协议，可以用极少的代码和带宽的为连接远程设备提供实时可靠的消息服务，它适用于硬件性能低下的远程设备以及网络状况糟糕的环境下，因此 MQTT 协议在 IoT（Internet of things，物联网），小型设备应用，移动应用等方面有较广泛的应用。 IoT 设备要运作，就必须连接到互联网，设备才能相互协作，以及与后端服务协同工作。而互联网的基础网络协议是 TCP/IP，MQTT 协议是基于 TCP/IP 协议栈而构建的，因此它已经慢慢的已经成为了 IoT 通讯的标准。 在简介完 MQTT 协议后，EMQ君将从其一些基本特点和基本概念为两部分，介绍 MQTT 协议。 ¶基本特点 MQTT 是一种 发布/订阅 传输协议，基本原理和实现如下： MQTT 协议提供一对多的消息发布，可以解除应用程序耦合，信息冗余小。该协议需要客户端和服务端，而协议中主要有三种身份：发布者（Publisher）、代理（Broker，服务器）、订阅者（Subscriber）。其中，消息的发布者和订阅者都是客户端，消息代理是服务器，而消息发布者可以同时是订阅者，实现了生产者与消费者的脱耦。 使用 TCP/IP 提供网络连接，提供有序、无损、双向连接；MQTT 是一种连接协议，它指定了如何组织数据字节并通过 TCP/IP 网络传输它们。设备联网，也需要连接到互联网中，在大万维的世界中，TCP 如同汽车，有轮子就能用来运输数据，MQTT 就像是交通规则。在网络模型中，TCP是传输层协议，而 MQTT是在应用层，在 TCP 的上层，因此MQTT 也是基于这个而构建的，提高了可靠性。 对负载内容屏蔽的消息传输；可以对消息订阅者所接受到的内容有所屏蔽。 具体有三种消息发布的服务质量（QoS） 至多一次（QoS0）：消息发布完全依赖底层 TCP/IP 网络。会发生消息丢失或重复。这一级别可用于如下情况，环境传感器数据，丢失一次读记录无所谓，因为不久后还会有第二次发送。 至少一次（QoS1）：确保消息到达，但消息重复可能会发生。 只有一次（QoS2）：确保消息到达一次。这一级别可用于如下情况，在计费系统中，消息重复或丢失会导致不正确的结果。 小型传输，开销小，固定长度的头部是 2 字节，协议交换最小化，以降低网络流量；整体上协议可拆分为：固定头部+可变头部+消息体，这就是为什么在介绍里说它非常适合 在物联网领域，传感器与服务器的通信，信息的收集。 使用 Last Will 和 Testament 特性通知有关各方客户端异常中断的机制； Last Will：即遗言机制，用于通知同一主题下的其他设备发送遗言的设备已经断开了连接。 Testament：遗嘱机制，功能类似于Last Will。 ¶基本术语 ¶网络连接 Network Connection MQTT使用的底层传输协议基础设施。 客户端使用它连接服务端。 它提供有序的、可靠的、双向字节流传输。 ¶应用消息 Application Message MQTT协议通过网络传输应用数据，应用消息通过MQTT传输时，它们有关联的服务质量（QoS）和主题（Topic）。 ¶客户端 Client 使用 MQTT 的程序或设备，客户端总是通过网络连接到服务端，它可以： 发布应用消息给其它相关的客户端。 订阅以请求接受相关的应用消息 取消订阅以移除接受应用消息的请求。 从服务端断开连接。 ¶服务端 Server 一个程序或设备，作为发送消息的客户端和请求订阅的客户端之间的中介，他需要： 接受来自客户端的网络连接 接受客户端发布的应用消息 处理客户端的订阅和取消订阅请求。 转发应用消息给符合条件的客户端订阅。 订阅 Subscription 订阅包含一个主题过滤器（Topic Filter）和一个最大的服务质量（QoS）等级。订阅与单个会话（Session）关联，会话可以包含多于一个的订阅，会话的每个订阅都有一个不同的主题过滤器。 ¶主题名 Topic Name 附加在应用消息上的一个标签，服务端已知且与订阅匹配，服务端发送应用消息的一个副本给每一个匹配的客户端订阅。 ¶主题过滤器 Topic Filter 订阅中包含的一个表达式，用于表示相关的一个或多个主题，主题过滤器可以使用通配符。 ¶会话 Session 客户端和服务端之间的状态交互，一些会话持续时长与网络连接一样，另一些可以在客户端和服务端的多个连续网络连接间扩展。 ¶控制报文 MQTT Control Packet 通过网络连接发送的信息数据包。MQTT规范定义了十四种不同类型的控制报文，其中一个（PUBLISH报文）用于传输应用消息。 ¶进一步了解 MQTT3 MQTT3（当前版本 3.1.1）是目前使用的最为广泛的 MQTT 协议标准。尽管 MQTT5 标准已经发布，并且带来了一些令人振奋的新特性，但是在整个应用场景上，从后台服务到消息中间件再到客户端 SDK 等环节上的产品升级并没有都完成，再加上既有部署的维护，业界从版本3到5的过渡可能会持续相当长一段时间，所以，对于刚加入物联网行业的生力军来说，现在来学习 MQTT 3 依然是一件很有意义的事情。 ¶MQTT 协议的工作方式 前面简介中讲到，在一个QMTT协议中有三个角色会参与到整个通信过程，发布者（publisher）、代理（broker）和订阅者（subscriber）。有别于传统的客户端/服务器通讯协议，MQTT协议并不是端到端的，消息传递通过代理，包括会话（session）也不是建立在发布者和订阅者之间，而是建立在端和代理之间。代理解除了发布者和订阅者之间的耦合。 除了发布者和订阅者之间传递普通消息，代理还可以为发布者处理保留消息和遗愿消息，并可以更改服务质量（QoS）等级。 ¶MQTT 控制报文格式 MQTT 协议通过交换预定义的MQTT控制报文来通信。这一节描述这些报文的格式。 MQTT 控制报文由三部分组成，如下： ¶固定报头 每个 MQTT 控制报文都 必须 包含一个固定报头，用于描述该报文具体行为。 ¶报文类型（协议名） 报文类型（协议名） 用首字节的四位比特（[7-4]）来分别表示 16 中报文类型（0 - 15），不同的值二进制表示方法如下： 协议名 值 报文流动方向 描述 Reserved 0 禁止 保留 CONNECT 1 客户端到服务端 客户端请求连接服务端 CONNACK 2 服务端到客户端 连接报文确认 PUBLISH 3 两个方向都允许 发布消息 PUBACK 4 两个方向都允许 QoS 1消息发布收到确认 PUBREC 5 两个方向都允许 发布收到（保证交付第一步） PUBREL 6 两个方向都允许 发布释放（保证交付第二步） PUBCOMP 7 两个方向都允许 QoS 2消息发布完成（保证交互第三步） SUBSCRIBE 8 客户端到服务端 客户端订阅请求 SUBACK 9 服务端到客户端 订阅请求报文确认 UNSUBSCRIBE 10 客户端到服务端 客户端取消订阅请求 UNSUBACK 11 服务端到客户端 取消订阅报文确认 PINGREQ 12 客户端到服务端 心跳请求 PINGRESP 13 服务端到客户端 心跳响应 DISCONNECT 14 客户端到服务端 客户端断开连接 Reserved 15 禁止 保留 ¶标志位 固定报头第1个字节的剩余的 4 位（[3-0]）包含每个 MQTT 控制报文类型特定的标志，见以下表格。表格中任何标记为“保留”的标志位，都是保留给以后使用的，必须 设置为表格中列出的值 [MQTT-2.2.2-1]。如果收到非法的标志，接收者 必须 关闭网络连接。 控制报文 固定报头标志 Bit 3 Bit 2 Bit 1 Bit 0 CONNECT Reserved 0 0 0 0 CONNACK Reserved 0 0 0 0 PUBLISH Used in MQTT 3.1.1 DUP1 QoS2 QoS2 RETAIN3 PUBACK Reserved 0 0 0 0 PUBREC Reserved 0 0 0 0 PUBREL Reserved 0 0 1 0 PUBCOMP Reserved 0 0 0 0 SUBSCRIBE Reserved 0 0 1 0 SUBACK Reserved 0 0 0 0 UNSUBSCRIBE Reserved 0 0 1 0 UNSUBACK Reserved 0 0 0 0 PINGREQ Reserved 0 0 0 0 PINGRESP Reserved 0 0 0 0 DISCONNECT Reserved 0 0 0 0 DUP1 =控制报文的重复分发标志 QoS2 = PUBLISH报文的服务质量等级 RETAIN3 = PUBLISH报文的保留标志 ¶剩余长度 剩余长度 表示当前报文后面还有多少字节，从第二个字节开始，使用 变长编码 方案来表示剩余长度，最多使用 4 个字节，最少使用 1 个字节。 什么是 变长编码 方案？ 变长编码，准确的说字节数与数值的大小有关，60 使用一个字节表示，180 却要使用 2 个字节表示。具体表示方法如下： 对小于 128 的值它使用单字节编码，更大的数值如下处理：每个字节只是用 低 7 位（[6 - 0]）来表示有效位，最高有效位（第 7 位）表示一个 真/假 标志，标志后面的字节是否也是表示长度。 例如：64 使用一个字节就可以表示（01000000）；但是 321 = 65 + 2 x 128 需要两个字节来表示（11000001 00000010），第一个表示 65 + 128 = 193，第二个字节为 2。 字节数 最小值 最大值 1 0 (0x00) 127 (0x7F) 2 128 (0x80, 0x01) 16 383 (0xFF, 0x7F) 3 16384 (0x80, 0x80, 0x01) 2 097 151 (0xFF, 0xFF, 0x7F) 4 2097152 (0x80, 0x80, 0x80, 0x01) 268 435 455 (0xFF, 0xFF, 0xFF, 0x7F) ¶可变报头 某些 MQTT 控制报文包含一个可变报头部分。它在固定报头和负载之间。可变报头的内容根据报文类型的不同而不同。可变报头的报文标识符（Packet Identifier）字段存在于在多个类型的报文里。 Bit 7 - 0 byte 1 报文标识符 MSB byte 2 报文标识符 LSB 很多控制报文的可变报头部分包含一个两字节的报文标识符字段。这些报文是 PUBLISH（QoS &gt; 0时）， PUBACK、PUBREC、PUBREL、PUBCOMP、SUBSCRIBE、SUBACK、UNSUBSCIBE、UNSUBACK。 SUBSCRIBE、UNSUBSCRIBE 和 PUBLISH（QoS大于0）控制报文 必须 包含一个非零的16位报文标识符（Packet Identifier）。客户端每次发送一个新的这些类型的报文时都 必须 分配一个当前未使用的报文标识符。如果一个客户端要重发这个特殊的控制报文，在随后重发那个报文时，它 必须 使用相同的标识符。当客户端处理完这个报文对应的确认后，这个报文标识符就释放可重用。QoS 1 的 PUBLISH 对应的是 PUBACK，QoS 2 的 PUBLISH 对应的是 PUBCOMP，与 SUBSCRIBE 或 UNSUBSCRIBE 对应的分别是 SUBACK 或 UNSUBACK。发送一个 QoS 0 的 PUBLISH 报文时，相同的条件也适用于服务端。 QoS等于 0 的 PUBLISH 报文 不能 包含报文标识符。 PUBACK、PUBREC、PUBREL 报文 必须 包含与最初发送的 PUBLISH 报文相同的报文标识符。类似地，SUBACK 和 UNSUBACK 必须 包含在对应的 SUBSCRIBE 和 UNSUBSCRIBE 报文中使用的报文标识符。 ¶有效载荷 具体协议内容，不同协议有不同的载荷。 ¶MQTT 控制报文详解 该部分内容，请查看我的另一篇博客 MQTT - 控制报文详情 ¶参考链接 MQTT Version 3.1.1 Official Manual]]></content>
      <categories>
        <category>MQTT</category>
      </categories>
      <tags>
        <tag>MQTT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitLab-CI-持续集成]]></title>
    <url>%2F2019%2F06%2F27%2F2019%2Fgitlab-ci-%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%2F</url>
    <content type="text"><![CDATA[本节讲述了如何配置 .gitlab-ci.yml，它为每个 Runner 的定制具体的任务。如果你想快速入手 从 GitLab 8.0 开始，GitLab CI 就已经集成在 GitLab 中，我们只要在项目中添加一个 .gitlab-ci.yml 文件，然后添加一个 Runner，即可进行持续集成。 而且随着 GitLab的升级，GitLab CI 变得越来越强大，本文将介绍如何使用 GitLab CI 进行持续集成。 ¶全局配置 从 gitlab 7.12，gitlab ci 使用 YAML 文件格式来作为项目的配置文件。 .gitlab-ci.yml 定义了每一个 runner 运行的状态，环境以及运行方式，每一个运行任务都必须要定义一个 script。 12345job1_name: script: "execute-script-for-job1"job2_name: script: "execute-script-for-job2" 上面是一个简单的 job 示例，每个 job 可以定义不同的执行脚本或命令。不管是并行运行的任务，还是串行运行的任务，他们之间都是相互绝对独立的。 .gitlab-ci.yml 一些保留字不能被用于作为 Job 的名字。 名字 是否必须 描述 image no 使用 docker 镜像 services no 使用 docker 服务 stages no 定义 build 级别 types no stages 的别名 before_script no 定义每个 job 执行前执行的脚本 after_script no 定义每个 job 执行后执行的脚本 variables no 定义全局环境变量 cache no 定义一些需要被缓存的文件 ¶image or services image or services 允许指定一个自定义的 docker 镜像或一系列的 docker 服务在执行 build 的时候，具体请参考详情 ¶stages stages 用于定义一系列的 build 级别，这些 级别 用于 job中，定义的所有级别按照从前往后的顺序执行 job。 级别之间的运行顺序： 相同 级别 的 job 都并行执行。 下一个级别的所有 job 都必须在上一个级别的所有 job 都 build success 的时候才能执行。 1234stages: - build - test - deploy 不同级别的 job 运行示例: 运行步骤： 所有的 build 级别的 job 都并行执行。 当所有的 build job 都执行 成功 后，所有的 test 级别的 job 开始并行执行。 当所有的 test job 都执行 成功 后，所有的 deploy 级别的 job 开始并行执行。 当所有的 deploy job 都执行 成功 后，这个 commit 被标记为 passed。 如果其中任何一个 job 执行失败，这个 commit 被标记为 failed，并且跳出执行，以后的所有 stage 的 job 都不会在执行。 PS: 假如 .gitlab-ci.yml 没有定义 stages，则默认顺序为 build -&gt; test -&gt; deploy 假如 job 没有定义 stage，则默认该任务的 stage 为 ‘test’ ¶types stages 的别名。 ¶before_script before_script 定义了一系列命令，他们将会在每一个 job 之前运行，他可以是一个数组，也可以是一个多行的字符串。 ¶after_script before_script 定义了一系列命令，他们将会在每一个 job 运行完成后执行，他可以是一个数组，也可以是一个多行的字符串。 ¶variables Gitlab CI 允许你定一些变量，他们将会被复制到当前进程的环境变量中。他们能在之后运行的 脚本 或 命令行 中引用，预先定义的变量。 1234variables: DATABASE_URL: "postgres://postgres@postgres/my_database" REDIS_URL: "unix:/etc/redis/sockets/redis.socket" NODE_ENV: "production" PS: 如果定义了 Job 级别的环境变量的话，该 Job 会优先使用 Job 级别的环境变量。 ¶cache 定义需要缓存的文件。每个 Job 开始的时候，Runner 都会删掉 .gitignore 里面的文件。如果有些文件 (如 node_modules/ ) 需要多个 Jobs 共用的话，我们只能让每个 Job 都先执行一遍 npm install。 这样很不方便，因此我们需要对这些文件进行缓存。缓存了的文件除了可以跨 Jobs 使用外，还可以跨 Pipeline 使用。 具体用法请查看官方文档。 PS： 默认 cache 在每一个 job 和 branch 是开启的。 如果定义 全局的 cache，他表示每一个 job 都会使用该 cache 以下为具体示例： 缓存 binaries 目录和 .config 文件。 123456job1: script: test cache: paths: - binaries/ - .config 缓存所有未跟踪（untracked）的文件。 1234job2: script: test cache: untracked: true 缓存所有未跟踪（untracked）的文件和 binaries 目录。 123456job2: script: test cache: untracked: true paths: - binaries/ job 级别的 cache 会重写 全局 cahce，下面示例只会缓存 binaries 目录。 123456789cache: paths: - my/filesjob1: script: test cache: paths: - binaries/ ¶cache:key key 字段允许你自由的定义 cache 的方式，是每个 job 都有一次缓存，还是 *所有 jobs * 只有一次 cache；它允许你在不同的 job 之间缓存数据；也允许你在不同的 branch 之间缓存数据。 To enable per-job caching: 123cache: key: "$CI_BUILD_NAME" untracked: true To enable per-branch caching: 123cache: key: "$CI_BUILD_REF_NAME" untracked: true To enable per-job and per-branch caching: 123cache: key: "$CI_BUILD_NAME/$CI_BUILD_REF_NAME" untracked: true To enable per-branch and per-stage caching: 123cache: key: "$CI_BUILD_STAGE/$CI_BUILD_REF_NAME" untracked: true If you use Windows Batch to run your shell scripts you need to replace $ with %: 123cache: key: "%CI_BUILD_STAGE%/%CI_BUILD_REF_NAME%" untracked: true ¶Job 配置 .gitlab-ci.yml 允许你无限制的定义 job，每个 job 都必须要有一个 唯一 的名字，但不能是上面所提到的关键字，同时还提供了一些参数来制定你的 job 行为。 12345678910111213job_name: script: - rake spec - coverage stage: test only: - master except: - develop tags: - ruby - postgres allow_failure: true 参数名 是否必须 描述 script Required 需要运行的脚本或命令 image no 使用 docker 镜像 services no 使用 docker 服务 stage no 定义 build 级别，默认 test type no stage 的别名 variables no 定义 job 级别的环境变量 only no 指定 job 哪些 branch 或 tags 是可以执行的 except no 指定 job 哪些 branch 或 tags 是可以不可用执行的 tags no 指定执行 job 的 runner allow_failure no 指定 job 允许失败 when no 定义任务执行时间 artifacts no 指定需要上传到 Gitlab 的文件 dependencies no 定义需要下载的 artifacts cache no 定义一些需要被缓存的文件 before_script no 指定 job 执行前运行的脚本或命令 after_script no 指定 job 执行完成后运行的脚本或命令 environment no 定义发布到指定的 environment 中 ¶script Runner 运行的所需的脚本，他可以是 字符串 或 数组，如下： 如果只运行一条命令时，可以选择字符串： 1script: "npm run build" 如果要运行多行命令，可以选择数组： 1234script: - uname -a - npm install - npm run build ¶image or services image or services 允许指定一个自定义的 docker 镜像或一系列的 docker 服务在执行 build 的时候，具体请参考详情。 PS：如果该值存在，在这个 job 中使用该值，否则使用全局定义的 image &amp; services ¶stage 指定当前 job 的级别，相同级别的 job 是并行执行的，详情参考 stages。 ¶type stage 的别名。 ¶variables job 级别 的变量，一旦定义了 job 级别 的变量，它所定义的变量将优先于 YAML 中定义的全局变量 和 预定义变量。 变量的优先级参考这里 ¶only or except only 或 except 参数可以指定一套规则来限制 job 是否需要执行。 可以通过配置 only 来指定哪些 branch 或 tag 是 需要 执行 job。 可以通过配置 only 来指定哪些 branch 或 tag 是 不需要 执行 job。 参数规则： only 和 except 会同时生效，如果同时指定了 only 和 except，他们都会生效。 only 和 except 都支持正则表达式。 only 和 except 都可以使用特殊关键字：branches、tags 和 triggers 。 only 和 except 可以根据仓库地址来过滤，一般用户 fork 工作流。 示例： only 规定了所有符合 /^issue\-.*$/ 的 branch 或 tag 都可以执行，except 规定了所有的 branch 都不执行，所以最终只执行了 所有符合正则表达式的 tag。 1234567job: # use regexp only: - /^issue-.*$/ # use special keyword except: - branches 只有 tags 和 triggers 执行。 12345job: # use special keywords only: - tags - triggers job 只执行父仓库，不执行 fork 的仓库，即：只执行 gitlab-org/gitlab-ce 上的 branches 且不执行 master 分支。 12345job: only: - branches@gitlab-org/gitlab-ce except: - master@gitlab-org/gitlab-ce ¶tags tags 用来指定特殊的 Runner 来运行这个 job。 tags 的过滤方式是通过 runner 在注册的时候会指定该 runner 的 tag。 示例：下面是一个 tag 为 shell 的 runner。 我们某一个 job 指定一个 runner 来运行这个 job。 123job: tags: - shell ¶allow_failure 当 job 运行失败或出现警告的时候，则依然认为该 job 是 成功 的，但是这个 build 详情里面能够看到相关的错误信息与警告信息。 ¶when 该参数指定 job 执行的条件，参数值可以取： on_success：只有之前的所有 job 都执行成功了，才执行这个 job，默认。 on_failure：只要之前的所有 job 中有一个执行失败了，就执行这个 job。 always：不管之前的所有 job 执行状态是什么，都执行这个 job。 manual：手动执行，请参考 manual actions。 12345678910111213141516171819202122232425262728293031323334stages: - build - cleanup_build - test - deploy - cleanupbuild_job: stage: build script: - make buildcleanup_build_job: stage: cleanup_build script: - cleanup build when failed when: on_failuretest_job: stage: test script: - make testdeploy_job: stage: deploy script: - make deploy when: manualcleanup_job: stage: cleanup script: - cleanup after builds when: always 上面的执行情况为： 当 build_job 执行失败，就会执行 cleanup_build_job，否则不执行。 无论如何 cleanup_job 都会作为最后一个 job 执行。 允许你从 Gitlab UI 中手动执行 deploy_job。 PS: manual 是一种特殊的类型，他类型阻止 job 自动运行，需要用户手动点击执行。 ¶artifacts 定义 Job 中生成的附件。当该 Job 运行成功后，生成的文件可以作为附件 (如生成的二进制文件) 保留下来，打包发送到 GitLab，之后我们可以在 GitLab 的项目页面下下载该附件。 注意：不要把 artifacts 和 cache 混淆了。 下面是一些具体示例： 发送所有位于 binaries 目录下的文件和 .config 文件。 1234artifacts: paths: - binaries/ - .config 发送所有 未跟踪 的文件。 12artifacts: untracked: true 发送所有位于 binaries 目录下的 未跟踪 文件。 1234artifacts: untracked: true paths: - binaries/ 当 tag 的时候，发送所有 binaries 目录下的文件。 12345678release-job: script: - mvn package -U artifacts: paths: - target/*.war only: - tags 发送所有 binaries 目录下的文件，并指定发送文件的名字。 12345job: artifacts: # 指定 name：job_name.current_branch_or_tag_name name: "$&#123;CI_BUILD_NAME&#125;.$&#123;CI_BUILD_REF_NAME&#125;" untracked: true 指定发送文件的条件。 123job: artifacts: when: on_failure when 的取值有： on_success：当 job 执行成功，就发送文件。 on_failure：当 job 执行失败，就发送文件。 always：无论 job 执行成功或者失败，都发送文件。 指定发送文件的 生存期。 expire_in 是指该文件能够保存的时长，比如： ‘3 mins 4 sec’ ‘2 hrs 20 min’ ‘2h20min’ ‘6 mos 1 day’ ‘47 yrs 6 mos and 4d’ ‘3 weeks and 2 days’ 123job: artifacts: expire_in: 1 week ¶dependencies 该参数用于与artifacts结合使用，允许你指定不同 job 之间 artifacts，的传输。 PS：默认后面的 job 将会下载前面所有 job 所上传的文件。 通过这个参数，能够有效的为每一个 job 定义他所需要下载的 artifacts，该参数的值 必须 为前面所定义好的 job 的名字（按照 stages 定义的顺序来判断），如果 dependencies 的值，在 stages 定义的顺序中，如果 job 的 dependencies 的值指向了一个前面没有定义过的 job 的名字则将会抛出错误。dependencies 指定 空数组则不会下载任何前面上传的 artifacts。 示例： 1234567891011121314151617181920212223242526272829build:osx: stage: build script: make build:osx artifacts: paths: - binaries/build:linux: stage: build script: make build:linux artifacts: paths: - binaries/test:osx: stage: test script: make test:osx dependencies: - build:osxtest:linux: stage: test script: make test:linux dependencies: - build:linuxdeploy: stage: deploy script: make deploy 在该示例中： test:osx 在 script 执行前下载 build:osx 上传的 artifacts。 test:linux 在 script 执行前下载 build:linux 上传的 artifacts。 deploy 在 script 执行前下载 build:osx 和 build:linux 上传的 artifacts。 ¶cache 该参数在当前 job 中将会重写 全局的 cache。 ¶before_script 该参数在当前 job 中将会重写 全局的 before_script。 ¶after_script 该参数在当前 job 中将会重写 全局的 after_script。 ¶environment 该值用来指定发布（deploy）的时候指定到一个特殊的 environment 中，这个将会非常有利于在 Gitlab 中追踪整个发布过程。 PS：environment 的值只能包含 字母、数字、- 和 _ 组成。 1234deploy_to_production: stage: deploy script: git push production HEAD:master environment: production ¶示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263stages: - install_deps - test - build - deploy_test - deploy_productioncache: key: $&#123;CI_BUILD_REF_NAME&#125; paths: - node_modules/ - dist/# 安装依赖install_deps: stage: install_deps only: - develop - master script: - npm install# 运行测试用例test: stage: test only: - develop - master script: - npm run test# 编译build: stage: build only: - develop - master script: - npm run clean - npm run build:client - npm run build:server# 部署测试服务器deploy_test: stage: deploy_test only: - develop script: - pm2 delete app || true - pm2 start app.js --name app# 部署生产服务器deploy_production: stage: deploy_production only: - master script: - bash scripts/deploy/deploy.sh 上面的配置把一次 Pipeline 分成五个阶段： 安装依赖(install_deps) 运行测试(test) 编译(build) 部署测试服务器(deploy_test) 部署生产服务器(deploy_production) 设置 Job.only 后，只有当 develop 分支和 master 分支有提交的时候才会触发相关的 Jobs。 注意，我这里用 GitLab Runner 所在的服务器作为测试服务器。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git 工作流程]]></title>
    <url>%2F2019%2F06%2F26%2F2019%2Fgit-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[有一套 规范、严谨 的开发流程是一个团队提高工作效率，减少工作冲突的重要途径，以下使我们公司目前使用的开发流程，请大家 务必严格 遵循该流程。 ¶流程说明 ¶分支概述 从上图，我们可以大概清楚整个开发流程，大概了解各个开发分支的作用与关系，下面我们将详细概述他们。 Git 分支一般分为两种：长期支持分支，短期支持分支。长期支持 的分支是受保护的，长期维护、迭代和更新的，是不能被删除的；短期支持 分支一般是用于实现某个新功能、修复长期支持分支上的问题而产生的，他们都是基于长期分支的小分支，在它们的使命完成后都需要被删除。 长期支持的分支有： master：主分支，负责记录上线版本的迭代，该分支代码与线上代码是完全一致的。 develop：开发分支，该分支记录相对稳定的版本，所有的 feature 分支和 bugfix 分支都从该分支创建。 短期支持分支有： feature/*：特性（功能）分支，用于开发新的功能，不同的功能创建不同的功能分支，功能分支开发完成并自测通过之后，需要合并到 develop 分支，之后删除该分支。 bugfix/*：bug 修复分支，用于修复不紧急的 bug，普通 bug 均需要创建 bugfix 分支开发，开发完成自测没问题后合并到 develop 分支后，删除该分支。 release/*：发布分支，用于代码上线准备，该分支从 develop 分支创建，创建之后由测试同学发布到测试环境进行测试，测试过程中发现 bug 需要开发人员在该release分支上进行bug修复，所有bug修复完后，在上线之前，需要合并该 release 分支到 master 分支和 develop 分支。 hotfix/*：紧急 bug 修复分支，该分支只有在紧急情况下使用，从 master 分支创建，用于紧急修复线上 bug，修复完成后，需要合并该分支到 master 分支以便上线，同时需要再合并到 develop 分支。 ¶长期支持分支 ¶master 一切源于 master。 一个丰富、复杂的代码仓库都是以 master 分支开始的，相信 git 用户都会非常了解这个分支，没错它就是我们的主分支，他的代码与线上的代码一模一样（这点，每个项目管理员都必须保证），所以他是稳定的、干净的、经过严格测试过的。 ¶develop develop 一开始是从 master 切出来复制版，它永远跑在 master 的前面，它包含了一切未来即将上线的功能。 所有 新功能 都是基于 develop 分支做开发，当然，部分小的修改可以直接在这个分支上提交代码，但提交者必须明确自己在做什么，必须对自己严谨，一般不推荐。 ¶短期支持分支 在主分支（长期支持分支）之后，我们使用了大量的短期分支，通过这些短期分支来协助开发，充分的利用 git 分支的特性，使每个分支的功能更加 明确 与 简单 ，不同于长期分支，这些分支都有一个较短的生命周期。 任何短期分支都有一个明确的目的，并且必须遵守严格的规则。 我们都必须遵循：所有的短期分支功能必须做到单一、明确，而不应该出现过于复杂的业务逻辑，更不应该跨业务。 ¶Feature Feature 分支用于开发新功能，俗称 功能分支，该分支完成后将会合并入 develop 分支，并删除。 该分支 必须 基于 develop ，分支功能完成后 必须 合并到 develop。 命名规范：除了 master、develop、release-*、bugfix-* 或 hotfix-* 之外的任何东西，最好名字能概括分支的功能，但不宜过长。 ¶示例 创建一个新功能分支。 12$ git checkout -t origin/develop -b myfeature# Switched to a new branch "myfeature" 提交更新。 12345$ git add -A$ git commit -m 'Your commit message here.'$ git fetch$ git pull --rebase$ git push origin myfeature -f 注意： - 一个分支一条 commit，如果后续有更新，请使用 git commit --amend 提交更改。 - push 代码前 必须 拉取远程最新代码，以防止其他人与产生冲突。 功能完成，将该分支合并到 develop 分支。 12345678$ git checkout develop# Switched to branch 'develop'$ git merge --no-ff myfeature# Updating ea1b82a..05e9557# (Summary of changes)$ git branch -d myfeature# Deleted branch myfeature (was 05e9557).$ git push origin develop --no-ff 选项为强制关闭 Fast-Forward 模式，即每次合并都会产生一个 commit，需要用户指定 message 内容，该操作可以避免丢失部分信息。 fast-forward 方式就是当条件允许的时候，git 直接把 HEAD 指针指向合并分支的头，完成合并。属于 快进方式，不过这种情况如果删除分支，则会丢失分支信息。因为在这个过程中没有创建 commit。 功能分支完成使命，删除。 12345$ git branch -D myfeature# Deleted branch myfeature (was ff452fe).$ git push origin :myfeature# To git@git.domain.com:group/project_name.git# - [deleted] myfeature ¶Release Release 分支用于准备即将上线的内容，此外，该分支可以对 版本号、构建日期 进行更改。通过 发布分支 来完成这些事情，develop 分支就继续接受下一次需要发布的功能、Bug 修复等等，从而不需要等待 master 发布完成后。 该分支 必须 基于 develop ，分支功能完成后 必须 合并到 develop 和 master。 命名规范：release-*。 ¶示例 例如：目前正式环境的版本为 1.1.5，而此时有两个功能分支 feature/A 和 feature/B，其中 feature/A 是本次需要部署到正式环境的内容，版本号为 1.2.0。而 feature/B 是准备在 1.3.0 中发布的新功能。所以，我们需要先将 feature/A 合并到 develop 分支，然后基于当前 develop 分支中创建一个 release-1.2 分支，这样 develop 就被释放了，他可以继续接受其他的新功能了，等待下一次发布。 创建发布分支。 发布分支是从 develop 分支创建的，创建的时候 务必 确保当前 develop 的所有功能都是本次上线的功能，并且是经过测试的，下一次上线的功能需要在发布分支创建之后再合并入 develop。 12$ git checkout -t origin/develop -b release-1.2.0# Switched to a new branch "release-1.2.0" 更新元信息：版本好、构建时间等等。 12345$ ./bump-version.sh 1.2.0# Files modified successfully, version bumped to 1.2.0.$ git commit -a -m "Bumped version number to 1.2.0"# [release-1.2.0 74d9424] Bumped version number to 1.2.0# 1 files changed, 1 insertions(+), 1 deletions(-) 合并到 master。 12345678$ git checkout master# Switched to branch 'master'$ git merge --no-ff release-1.2.0# Merge made by recursive.# (Summary of changes)$ git push origin master$ git tag -a 1.2.0$ git push origin 1.2.0 合并到 develop。 12345$ git checkout develop# Switched to branch 'develop'$ git merge --no-ff release-1.2.0# Merge made by recursive.# (Summary of changes) 删除发布分支。 12$ git branch -D release-1.2.0# Deleted branch release-1.2.0 (was ff452fe). ¶Hotfix Hotfix 分支跟 Release 分支差不多，都是为了准备下一次上线内容，不过他用于处理 紧急的 的线上环境的 Bug，Hotfix 分支必须从 master 分支创建。 Hotfix 分支的本质作用为：当一个人紧急修复正式环境某个 Bug 时不会影响其他开发人员（develop 分支）的工作进度。 该分支 必须 基于 master ，分支功能完成后 必须 合并到 develop 和 master 。 命名规范：hotfix-*。 ¶示例 例如：目前正式环境的版本为 1.2.0，但是，发现线上有一些 Bug，需要紧急修复，但是 develop 分支上的新功能还不稳定，不能发布，所以我们需要使用 hotfix 分支来修复线上 Bug。 创建 hotfix 分支。 12$ git checkout -t origin/master -b hotfix-1.2.1Switched to a new branch "hotfix-1.2.1" 修复 Bug 之后，提交更改。 123$ git commit -m "Fixed severe production problem"# [hotfix-1.2.1 abbe5d6] Fixed severe production problem# 5 files changed, 32 insertions(+), 17 deletions(-) 提升版本号，然后提交。 12345$ ./bump-version.sh 1.2.1# Files modified successfully, version bumped to 1.2.1.$ git commit -a -m "Bumped version number to 1.2.1"# [hotfix-1.2.1 41e61bb] Bumped version number to 1.2.1# 1 files changed, 1 insertions(+), 1 deletions(-) 合并到 master。 将修复过的程序发布到正式环境。 12345678$ git checkout master# Switched to branch 'master'$ git merge --no-ff hotfix-1.2.1# Merge made by recursive.# (Summary of changes)$ git push origin master$ git tag -a 1.2.1$ git push origin 1.2.1 合并到 develop。 为了确保下一次发布包含该修复的问题，我们需要将该 hotfix 分支合并入 develop 分支。 12345$ git checkout develop# Switched to branch 'develop'$ git merge --no-ff hotfix-1.2.1# Merge made by recursive.# (Summary of changes) 注意：如果有 未发布 的 发布分支 存在，则需要将该 hotfix 分支合并入 发布分支，而不是 develop 分支，因为最终发布分支也会被合并到 develop 分支，如果 develop 非常紧急的需要使用 hotfix 上修复的内容，也可以合并到 develop 分支，这都不会有影响。 删除该分支。 该功能分支使命结束后，删除它。 12$ git branch -d hotfix-1.2.1# Deleted branch hotfix-1.2.1 (was abbe5d6). ¶版本标记 同大多数 VCS 一样，Git 也可以对某一时间点上的版本打上标签。人们在发布某个软件版本（比如 v1.0 等等）的时候，经常这么做。 Git 使用的标签有两种类型： 轻量级的（lightweight）：轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。 含附注的（annotated）：含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。一般我们都建议使用含附注型的标签，以便保留相关信息； 当然，如果只是临时性加注标签，或者不需要旁注额外信息，用 轻量级标签 也没问题。 详情查看 - 如何打标签？ ¶使用 git flow 简化操作 git flow 是 git 的一个插件，可以极大程度的简化执行 git 标准分支流程的操作，可以在 gitflow-avh 安装。 详情参考： Git 相关工具介绍 - Git flow。 ¶特别鸣谢 A successful Git branching model - Vincent Driessen 研发团队GIT开发流程新人学习指南 - 管宜尧 Git 分支 - 利用分支进行开发的工作流程 Git分支管理策略 - 阮一峰]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-基本知识]]></title>
    <url>%2F2019%2F06%2F25%2F2019%2Fgit-%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[这篇指南以大家在SVN中已经广为熟悉使用的集中式工作流作为起点，循序渐进地演进到其它高效的分布式工作流，还介绍了如何配合使用便利的Pull Request功能，体系地讲解了各种工作流的应用。 如果你Git用的还不多，可以从前面的讲的工作流开始操练。操作过程去感受指南的讲解：解决什么问题、如何解决问题，这样理解就深了，也方便活用。 GIT 是目前主流的分布式版本控制系统。 GIT 详细文档点击这里 ¶简介 行文中实践原则和操作示例并重，对于Git的资深玩家可以梳理思考提升，而新接触的同学，也可以跟着step-by-step操练学习并在实际工作中上手使用。 工作流其实不是一个初级主题，背后的本质问题其实是 有效的项目流程管理 和 高效的开发协同约定，而不仅是Git或SVN等VCS或SCM工具的使用。 关于Git工作流主题，网上体系的中文资料不多，主要是零散的操作说明，希望这篇文章能让你更深入理解并在工作中灵活有效地使用起来。 Gitflow工作流是经典模型，处于核心位置，体现了工作流的经验和精髓。随着项目过程复杂化，你会感受到这个工作流中的深思熟虑和威力！ Forking工作流是分布式协作的（GitHub风格）可以先看看GitHub的Help：Fork A Repo和Using pull requests 。照着操作，给一个GitHub项目贡献你的提交，有操作经验再看指南容易意会。指南中给了自己实现Fork的方法：Fork就是服务端的克隆。在指南的操练中使用代码托管服务（如GitHub、Bitbucket），可以点一下按钮就让开发者完成仓库的fork操作。 PS： 文中Pull Request的介绍用的是Bitbucket代码托管服务，由于和GITLAB基本一样，如果你用的是GITLAB（我自己也主要使用GITLAB托管代码），不影响理解和操作。 ¶Git 工作流指南 工作流有各式各样的用法，但也正因此使得在实际工作中如何上手使用变得很头大。这篇指南通过总览公司团队中最常用的几种Git工作流让大家可以上手使用。 在阅读的过程中请记住，本文中的几种工作流是作为方案指导而不是条例规定。在展示了各种工作流可能的用法后，你可以从不同的工作流中挑选或揉合出一个满足你自己需求的工作流。 ¶集中式工作流 如果你的开发团队成员已经很熟悉Subversion，集中式工作流让你无需去适应一个全新流程就可以体验Git带来的收益。这个工作流也可以作为向更Git风格工作流迁移的友好过渡。 转到分布式版本控制系统看起来像个令人生畏的任务，但不改变已用的工作流你也可以用上Git带来的收益。团队可以用和Subversion完全不变的方式来开发项目。 但使用Git加强开发的工作流，Git有相比SVN的几个优势。 首先，每个开发可以有属于自己的整个工程的本地拷贝。隔离的环境让各个开发者的工作和项目的其他部分修改独立开来 —— 即自由地提交到自己的本地仓库，先完全忽略上游的开发，直到方便的时候再把修改反馈上去。 其次，Git提供了强壮的分支和合并模型。不像SVN，Git的分支设计成可以做为一种用来在仓库之间集成代码和分享修改的『失败安全』的机制。 ¶工作方式 像Subversion一样，集中式工作流以中央仓库作为项目所有修改的单点实体。相比SVN缺省的开发分支trunk，Git叫做master，所有修改提交到这个分支上。本工作流只用到master这一个分支。 开发者开始先克隆中央仓库。在自己的项目拷贝中像SVN一样的编辑文件和提交修改；但修改是存在本地的，和中央仓库是完全隔离的。开发者可以把和上游的同步延后到一个方便时间点。 要发布修改到正式项目中，开发者要把本地master分支的修改『推』到中央仓库中。这相当于svn commit操作，但push操作会把所有还不在中央仓库的本地提交都推上去。 ¶冲突解决 中央仓库代表了正式项目，所以提交历史应该被尊重且是稳定不变的。如果开发者本地的提交历史和中央仓库有分歧，Git会拒绝push提交否则会覆盖已经在中央库的正式提交。 在开发者提交自己功能修改到中央库前，需要先fetch在中央库的新增提交，rebase自己提交到中央库提交历史之上。 这样做的意思是在说，『我要把自己的修改加到别人已经完成的修改上。』最终的结果是一个完美的线性历史，就像以前的SVN的工作流中一样。 如果本地修改和上游提交有冲突，Git会暂停rebase过程，给你手动解决冲突的机会。Git解决合并冲突，用和生成提交一样的git status和git add命令，很一致方便。还有一点，如果解决冲突时遇到麻烦，Git可以很简单中止整个rebase操作，重来一次（或者让别人来帮助解决）。 ¶示例 让我们一起逐步分解来看看一个常见的小团队如何用这个工作流来协作的。有两个开发者小明和小红，看他们是如何开发自己的功能并提交到中央仓库上的。 ¶有人先初始化好中央仓库 第一步，有人在服务器上创建好中央仓库。如果是新项目，你可以初始化一个空仓库；否则你要导入已有的Git或SVN仓库。 中央仓库应该是个裸仓库（bare repository），即没有工作目录（working directory）的仓库。可以用下面的命令创建： 12ssh user@hostgit init --bare /path/to/repo.git 确保写上有效的user（SSH的用户名），host（服务器的域名或IP地址），/path/to/repo.git（你想存放仓库的位置）。 注意，为了表示是一个裸仓库，按照约定加上.git扩展名到仓库名上。 ¶所有人克隆中央仓库 下一步，各个开发者创建整个项目的本地拷贝。通过git clone命令完成： 1git clone ssh://user@host/path/to/repo.git 基于你后续会持续和克隆的仓库做交互的假设，克隆仓库时Git会自动添加远程别名origin指回『父』仓库。 ¶小明开发功能 在小明的本地仓库中，他使用标准的Git过程开发功能：编辑、暂存（Stage）和提交。 如果你不熟悉暂存区（Staging Area），这里说明一下：暂存区用来准备一个提交，但可以不用把工作目录中所有的修改内容都包含进来。 这样你可以创建一个高度聚焦的提交，尽管你本地修改很多内容。 123git status # 查看本地仓库的修改状态git add # 暂存文件git commit # 提交文件 请记住，因为这些命令生成的是本地提交，小明可以按自己需求反复操作多次，而不用担心中央仓库上有了什么操作。 对需要多个更简单更原子分块的大功能，这个做法是很有用的。 ¶小红开发功能 与此同时，小红在自己的本地仓库中用相同的编辑、暂存和提交过程开发功能。和小明一样，她也不关心中央仓库有没有新提交； 当然更不关心小明在他的本地仓库中的操作，因为所有本地仓库都是私有的。 ¶小明发布功能 一旦小明完成了他的功能开发，会发布他的本地提交到中央仓库中，这样其它团队成员可以看到他的修改。他可以用下面的git push命令： 1git push origin master 注意，origin是在小明克隆仓库时Git创建的远程中央仓库别名。master参数告诉Git推送的分支。 由于中央仓库自从小明克隆以来还没有被更新过，所以push操作不会有冲突，成功完成。 ¶小红试着发布功能 一起来看看在小明发布修改后，小红push修改会怎么样？她使用完全一样的push命令： 1git push origin master 但她的本地历史已经和中央仓库有分岐了，Git拒绝操作并给出下面很长的出错消息： 12345error: failed to push some refs to &apos;/path/to/repo.git&apos;hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Merge the remote changes (e.g. &apos;git pull&apos;)hint: before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 这避免了小红覆写正式的提交。她要先pull小明的更新到她的本地仓库合并上她的本地修改后，再重试。 ¶小红在小明的提交之上rebase 小红用git pull合并上游的修改到自己的仓库中。 这条命令类似svn update——拉取所有上游提交命令到小红的本地仓库，并尝试和她的本地修改合并： 1git pull --rebase origin master --rebase选项告诉Git把小红的提交移到同步了中央仓库修改后的master分支的顶部，如下图所示： 如果你忘加了这个选项，pull操作仍然可以完成，但每次pull操作要同步中央仓库中别人修改时，提交历史会以一个多余的『合并提交』结尾。 对于集中式工作流，最好是使用rebase而不是生成一个合并提交。 ¶小红解决合并冲突 rebase操作过程是把本地提交一次一个地迁移到更新了的中央仓库master分支之上。 这意味着可能要解决在迁移某个提交时出现的合并冲突，而不是解决包含了所有提交的大型合并时所出现的冲突。 这样的方式让你尽可能保持每个提交的聚焦和项目历史的整洁。反过来，简化了哪里引入Bug的分析，如果有必要，回滚修改也可以做到对项目影响最小。 如果小红和小明的功能是不相关的，不大可能在rebase过程中有冲突。如果有，Git在合并有冲突的提交处暂停rebase过程，输出下面的信息并带上相关的指令： 1CONFLICT (content): Merge conflict in &lt;some-file&gt; Git很赞的一点是，任何人可以解决他自己的冲突。在这个例子中，小红可以简单的运行git status命令来查看哪里有问题。 冲突文件列在Unmerged paths（未合并路径）一节中： 12345# Unmerged paths:# (use &quot;git reset HEAD &lt;some-file&gt;...&quot; to unstage)# (use &quot;git add/rm &lt;some-file&gt;...&quot; as appropriate to mark resolution)## both modified: &lt;some-file&gt; 接着小红编辑这些文件。修改完成后，用老套路暂存这些文件，并让git rebase完成剩下的事： 12git add &lt;some-file&gt; git rebase --continue 要做的就这些了。Git会继续一个一个地合并后面的提交，如其它的提交有冲突就重复这个过程。 如果你碰到了冲突，但发现搞不定，不要惊慌。只要执行下面这条命令，就可以回到你执行git pull --rebase命令前的样子： 1git rebase --abort ¶小红成功发布功能 小红完成和中央仓库的同步后，就能成功发布她的修改了： 1git push origin master 如你所见，仅使用几个Git命令我们就可以模拟出传统Subversion开发环境。对于要从SVN迁移过来的团队来说这太好了，但没有发挥出Git分布式本质的优势。 如果你的团队适应了集中式工作流，但想要更流畅的协作效果，绝对值得探索一下 功能分支工作流 的收益。 通过为一个功能分配一个专门的分支，能够做到一个新增功能集成到正式项目之前对新功能进行深入讨论。 ¶功能分支工作流 功能分支工作流以集中式工作流为基础，不同的是为各个新功能分配一个专门的分支来开发。这样可以在把新功能集成到正式项目前，用Pull Requests的方式讨论变更。 一旦你玩转了集中式工作流，在开发过程中可以很简单地加上功能分支，用来鼓励开发者之间协作和简化交流。 功能分支工作流背后的核心思路是所有的功能开发应该在一个专门的分支，而不是在master分支上。 这个隔离可以方便多个开发者在各自的功能上开发而不会弄乱主干代码。 另外，也保证了master分支的代码一定不会是有问题的，极大有利于集成环境。 功能开发隔离也让pull requests工作流成功可能， pull requests工作流能为每个分支发起一个讨论，在分支合入正式项目之前，给其它开发者有表示赞同的机会。 另外，如果你在功能开发中有问题卡住了，可以开一个pull requests来向同学们征求建议。 这些做法的重点就是，pull requests让团队成员之间互相评论工作变成非常方便！ ¶工作方式 功能分支工作流仍然用中央仓库，并且master分支还是代表了正式项目的历史。 但不是直接提交本地历史到各自的本地master分支，开发者每次在开始新功能前先创建一个新分支。 功能分支应该有个有描述性的名字，比如animated-menu-items或issue-#1061，这样可以让分支有个清楚且高聚焦的用途。 在master分支和功能分支之间，Git是没有技术上的区别，所以开发者可以用和集中式工作流中完全一样的方式编辑、暂存和提交修改到功能分支上。 另外，功能分支也可以（且应该）push到中央仓库中。这样不修改正式代码就可以和其它开发者分享提交的功能。 由于master是仅有的一个『特殊』分支，在中央仓库上存多个功能分支不会有任何问题。当然，这样做也可以很方便地备份各自的本地提交。 ¶Pull Requests 功能分支除了可以隔离功能的开发，也使得通过Pull Requests讨论变更成为可能。 一旦某个开发完成一个功能，不是立即合并到master，而是push到中央仓库的功能分支上并发起一个Pull Request请求去合并修改到master。 在修改成为主干代码前，这让其它的开发者有机会先去Review变更。 Code Review是Pull Requests的一个重要的收益，但Pull Requests目的是讨论代码一个通用方式。 你可以把Pull Requests作为专门给某个分支的讨论。这意味着可以在更早的开发过程中就可以进行Code Review。 比如，一个开发者开发功能需要帮助时，要做的就是发起一个Pull Request，相关的人就会自动收到通知，在相关的提交旁边能看到需要帮助解决的问题。 一旦Pull Request被接受了，发布功能要做的就和集中式工作流就很像了。 首先，确定本地的master分支和上游的master分支是同步的。然后合并功能分支到本地master分支并push已经更新的本地master分支到中央仓库。 仓库管理的产品解决方案像Bitbucket或Stash，可以良好地支持Pull Requests。可以看看Stash的Pull Requests文档。 ¶示例 下面的示例演示了如何把Pull Requests作为Code Review的方式，但注意Pull Requests可以用于很多其它的目的。 ¶小红开始开发一个新功能 在开始开发功能前，小红需要一个独立的分支。使用下面的命令新建一个分支： 1git checkout -b marys-feature master 这个命令检出一个基于master名为marys-feature的分支，Git的-b选项表示如果分支还不存在则新建分支。 这个新分支上，小红按老套路编辑、暂存和提交修改，按需要提交以实现功能： 123git statusgit add &lt;some-file&gt;git commit ¶小红要去吃个午饭 早上小红为新功能添加一些提交。 去吃午饭前，push功能分支到中央仓库是很好的做法，这样可以方便地备份，如果和其它开发协作，也让他们可以看到小红的提交。 1git push -u origin marys-feature 这条命令push marys-feature分支到中央仓库（origin），-u选项设置本地分支去跟踪远程对应的分支。 设置好跟踪的分支后，小红就可以使用git push命令省去指定推送分支的参数。 ¶小红完成功能开发 小红吃完午饭回来，完成整个功能的开发。在合并到master之前， 她发起一个Pull Request让团队的其它人知道功能已经完成。但首先，她要确认中央仓库中已经有她最近的提交： 1git push 然后，在她的Git GUI客户端中发起Pull Request，请求合并marys-feature到master，团队成员会自动收到通知。 Pull Request很酷的是可以在相关的提交旁边显示评注，所以你可以对某个变更集提问。 ¶小黑收到 Pull Request 小黑收到了Pull Request后会查看marys-feature的修改。决定在合并到正式项目前是否要做些修改，且通过Pull Request和小红来回地讨论。 ¶小红再做修改 要再做修改，小红用和功能第一个迭代完全一样的过程。编辑、暂存、提交并push更新到中央仓库。小红这些活动都会显示在Pull Request上，小黑可以断续做评注。 如果小黑有需要，也可以把marys-feature分支拉到本地，自己来修改，他加的提交也会一样显示在Pull Request上。 ¶小红发布她的功能 一旦小黑可以的接受Pull Request，就可以合并功能到稳定项目代码中（可以由小黑或是小红来做这个操作）： 1234git checkout mastergit pullgit pull origin marys-featuregit push 无论谁来做合并，首先要检出master分支并确认是它是最新的。然后执行git pull origin marys-feature合并marys-feature分支到和已经和远程一致的本地master分支。 你可以使用简单git merge marys-feature命令，但前面的命令可以保证总是最新的新功能分支。 最后更新的master分支要重新push回到origin。 这个过程常常会生成一个合并提交。有些开发者喜欢有合并提交，因为它像一个新功能和原来代码基线的连通符。 但如果你偏爱线性的提交历史，可以在执行合并时rebase新功能到master分支的顶部，这样生成一个快进（fast-forward）的合并。 一些GUI客户端可以只要点一下『接受』按钮执行好上面的命令来自动化Pull Request接受过程。 如果你的不能这样，至少在功能合并到master分支后能自动关闭Pull Request。 ¶与此同时，小明在做和小红一样的事 当小红和小黑在marys-feature上工作并讨论她的Pull Request的时候，小明在自己的功能分支上做完全一样的事。 通过隔离功能到独立的分支上，每个人都可以自主的工作，当然必要的时候在开发者之间分享变更还是比较繁琐的。 到了这里，但愿你发现了功能分支可以很直接地在 集中式工作流 的仅有的master分支上完成多功能的开发。 另外，功能分支还使用了Pull Request，使得可以在你的版本控制GUI客户端中讨论某个提交。 功能分支工作流是开发项目异常灵活的方式。问题是，有时候太灵活了。对于大型团队，常常需要给不同分支分配一个更具体的角色。 Gitflow工作流是管理功能开发、发布准备和维护的常用模式。 ¶Gitflow 工作流 Gitflow工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。 这节介绍的Gitflow工作流借鉴自在nvie的Vincent Driessen。 Gitflow工作流定义了一个围绕项目发布的严格分支模型。虽然比功能分支工作流复杂几分，但提供了用于一个健壮的用于管理大型项目的框架。 Gitflow工作流没有用超出功能分支工作流的概念和命令，而是为不同的分支分配一个很明确的角色，并定义分支之间如何和什么时候进行交互。 除了使用功能分支，在做准备、维护和记录发布也使用各自的分支。 当然你可以用上功能分支工作流所有的好处：Pull Requests、隔离实验性开发和更高效的协作。 ¶工作方式 Gitflow工作流仍然用中央仓库作为所有开发者的交互中心。和其它的工作流一样，开发者在本地工作并push分支到要中央仓库中。 ¶历史分支 相对使用仅有的一个master分支，Gitflow工作流使用2个分支来记录项目的历史。master分支存储了正式发布的历史，而develop分支作为功能的集成分支。 这样也方便master分支上的所有提交分配一个版本号。 剩下要说明的问题围绕着这2个分支的区别展开。 ¶功能分支 每个新功能位于一个自己的分支，这样可以push到中央仓库以备份和协作。 但功能分支不是从master分支上拉出新分支，而是使用develop分支作为父分支。当新功能完成时，合并回develop分支。 新功能提交应该从不直接与master分支交互。 注意，从各种含义和目的上来看，功能分支加上develop分支就是功能分支工作流的用法。但Gitflow工作流没有在这里止步。 ¶发布分支 一旦develop分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从develop分支上fork一个发布分支。 新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上—— 这个分支只应该做Bug修复、文档生成和其它面向发布任务。 一旦对外发布的工作都完成了，发布分支合并到master分支并分配一个版本号打好Tag。 另外，这些从新建发布分支以来的做的修改要合并回develop分支。 使用一个用于发布准备的专门分支，使得一个团队可以在完善当前的发布版本的同时，另一个团队可以继续开发下个版本的功能。 这也打造定义良好的开发阶段（比如，可以很轻松地说，『这周我们要做准备发布版本4.0』，并且在仓库的目录结构中可以实际看到）。 常用的分支约定： 123用于新建发布分支的分支: develop用于合并的分支: master分支命名: release-* 或 release/* ¶维护分支 维护分支或说是热修复（hotfix）分支用于生成快速给产品发布版本（production releases）打补丁，这是唯一可以直接从master分支fork出来的分支。 修复完成，修改应该马上合并回master分支和develop分支（当前的发布分支），master分支应该用新的版本号打好Tag。 为Bug修复使用专门分支，让团队可以处理掉问题而不用打断其它工作或是等待下一个发布循环。 你可以把维护分支想成是一个直接在master分支上处理的临时发布。 ¶示例 下面的示例演示本工作流如何用于管理单个发布循环。假设你已经创建了一个中央仓库。 ¶创建开发分支 第一步为master分支配套一个develop分支。简单来做可以本地创建一个空的develop分支，push到服务器上： 12git branch developgit push -u origin develop 以后这个分支将会包含了项目的全部历史，而master分支将只包含了部分历史。其它开发者这时应该克隆中央仓库，建好develop分支的跟踪分支： 12git clone ssh://user@host/path/to/repo.gitgit checkout -b develop origin/develop 现在每个开发都有了这些历史分支的本地拷贝。 ¶小红和小明开始开发新功能 这个示例中，小红和小明开始各自的功能开发。他们需要为各自的功能创建相应的分支。新分支不是基于master分支，而是应该基于develop分支： 1git checkout -b some-feature develop 他们用老套路添加提交到各自功能分支上：编辑、暂存、提交： 123git statusgit add &lt;some-file&gt;git commit ¶小红完成功能开发 添加了提交后，小红觉得她的功能OK了。如果团队使用Pull Requests，这时候可以发起一个用于合并到develop分支。 否则她可以直接合并到她本地的develop分支后push到中央仓库： 12345git pull origin developgit checkout developgit merge some-featuregit pushgit branch -d some-feature 第一条命令在合并功能前确保develop分支是最新的。注意，功能决不应该直接合并到master分支。 冲突解决方法和集中式工作流一样。 ¶小红开始准备发布 这个时候小明正在实现他的功能，小红开始准备她的第一个项目正式发布。 像功能开发一样，她用一个新的分支来做发布准备。这一步也确定了发布的版本号： 1git checkout -b release-0.1 develop 这个分支是清理发布、执行所有测试、更新文档和其它为下个发布做准备操作的地方，像是一个专门用于改善发布的功能分支。 只要小红创建这个分支并push到中央仓库，这个发布就是功能冻结的。任何不在develop分支中的新功能都推到下个发布循环中。 ¶小红完成发布 一旦准备好了对外发布，小红合并修改到master分支和develop分支上，删除发布分支。合并回develop分支很重要，因为在发布分支中已经提交的更新需要在后面的新功能中也要是可用的。 另外，如果小红的团队要求Code Review，这是一个发起Pull Request的理想时机。 1234567git checkout mastergit merge release-0.1git pushgit checkout developgit merge release-0.1git pushgit branch -d release-0.1 发布分支是作为功能开发（develop分支）和对外发布（master分支）间的缓冲。只要有合并到master分支，就应该打好Tag以方便跟踪。 12git tag -a 0.1 -m "Initial public release" mastergit push --tags Git有提供各种勾子（hook），即仓库有事件发生时触发执行的脚本。 可以配置一个勾子，在你push中央仓库的master分支时，自动构建好对外发布。 ¶最终用户发现Bug 对外发布后，小红回去和小明一起做下个发布的新功能开发，直到有最终用户开了一个Ticket抱怨当前版本的一个Bug。 为了处理Bug，小红（或小明）从master分支上拉出了一个维护分支，提交修改以解决问题，然后直接合并回master分支： 12345git checkout -b issue-#001 master# Fix the buggit checkout mastergit merge issue-#001git push 就像发布分支，维护分支中新加这些重要修改需要包含到develop分支中，所以小红要执行一个合并操作。然后就可以安全地删除这个分支了： 1234git checkout developgit merge issue-#001git pushgit branch -d issue-#001 到了这里，但愿你对集中式工作流、功能分支工作流和Gitflow工作流已经感觉很舒适了。 你应该也牢固的掌握了本地仓库的潜能，push/pull模式和Git健壮的分支和合并模型。 记住，这里演示的工作流只是可能用法的例子，而不是在实际工作中使用Git不可违逆的条例。 所以不要畏惧按自己需要对工作流的用法做取舍。不变的目标就是让Git为你所用。 ¶Forking 工作流 Forking工作流是分布式工作流，充分利用了Git在分支和克隆上的优势。可以安全可靠地管理大团队的开发者（developer），并能接受不信任贡献者（contributor）的提交。 Forking工作流和前面讨论的几种工作流有根本的不同，这种工作流不是使用单个服务端仓库作为『中央』代码基线，而让各个开发者都有一个服务端仓库。这意味着各个代码贡献者有2个Git仓库而不是1个：一个本地私有的，另一个服务端公开的。 Forking工作流的一个主要优势是，贡献的代码可以被集成，而不需要所有人都能push代码到仅有的中央仓库中。 开发者push到自己的服务端仓库，而只有项目维护者才能push到正式仓库。 这样项目维护者可以接受任何开发者的提交，但无需给他正式代码库的写权限。 效果就是一个分布式的工作流，能为大型、自发性的团队（包括了不受信的第三方）提供灵活的方式来安全的协作。 也让这个工作流成为开源项目的理想工作流。 ¶工作方式 和其它的Git工作流一样，Forking工作流要先有一个公开的正式仓库存储在服务器上。 但一个新的开发者想要在项目上工作时，不是直接从正式仓库克隆，而是fork正式项目在服务器上创建一个拷贝。 这个仓库拷贝作为他个人公开仓库 —— 其它开发者不允许push到这个仓库，但可以pull到修改（后面我们很快就会看这点很重要）。 在创建了自己服务端拷贝之后，和之前的工作流一样，开发者执行git clone命令克隆仓库到本地机器上，作为私有的开发环境。 要提交本地修改时，push提交到自己公开仓库中 —— 而不是正式仓库中。 然后，给正式仓库发起一个pull request，让项目维护者知道有更新已经准备好可以集成了。 对于贡献的代码，pull request也可以很方便地作为一个讨论的地方。 为了集成功能到正式代码库，维护者pull贡献者的变更到自己的本地仓库中，检查变更以确保不会让项目出错， 合并变更到自己本地的master分支， 然后pushmaster分支到服务器的正式仓库中。 到此，贡献的提交成为了项目的一部分，其它的开发者应该执行pull操作与正式仓库同步自己本地仓库。 ¶正式仓库 在Forking工作流中，『官方』仓库的叫法只是一个约定，理解这点很重要。 从技术上来看，各个开发者仓库和正式仓库在Git看来没有任何区别。 事实上，让正式仓库之所以正式的唯一原因是它是项目维护者的公开仓库。 ¶Forking 工作流的分支使用方式 所有的个人公开仓库实际上只是为了方便和其它的开发者共享分支。 各个开发者应该用分支隔离各个功能，就像在功能分支工作流和Gitflow工作流一样。 唯一的区别是这些分支被共享了。在Forking工作流中这些分支会被pull到另一个开发者的本地仓库中，而在功能分支工作流和Gitflow工作流中是直接被push到正式仓库中。 ¶示例 ¶项目维护者初始化正式仓库 和任何使用Git项目一样，第一步是创建在服务器上一个正式仓库，让所有团队成员都可以访问到。 通常这个仓库也会作为项目维护者的公开仓库。 公开仓库应该是裸仓库，不管是不是正式代码库。 所以项目维护者会运行像下面的命令来搭建正式仓库： 12ssh user@hostgit init --bare /path/to/repo.git Bitbucket和Stash提供了一个方便的GUI客户端以完成上面命令行做的事。 这个搭建中央仓库的过程和前面提到的工作流完全一样。 如果有现存的代码库，维护者也要push到这个仓库中。 ¶开发者fork正式仓库 其它所有的开发需要fork正式仓库。 可以用git clone命令用SSH协议连通到服务器， 拷贝仓库到服务器另一个位置 —— 是的，fork操作基本上就只是一个服务端的克隆。 Bitbucket和Stash上可以点一下按钮就让开发者完成仓库的fork操作。 这一步完成后，每个开发都在服务端有一个自己的仓库。和正式仓库一样，这些仓库应该是裸仓库。 ¶开发者克隆自己fork出来的仓库 下一步，各个开发者要克隆自己的公开仓库，用熟悉的git clone命令。 在这个示例中，假定用Bitbucket托管了仓库。记住，如果这样的话各个开发者需要有各自的Bitbucket账号， 使用下面命令克隆服务端自己的仓库： 1git clone https://user@bitbucket.org/user/repo.git 相比前面介绍的工作流只用了一个origin远程别名指向中央仓库，Forking工作流需要2个远程别名 —— 一个指向正式仓库，另一个指向开发者自己的服务端仓库。别名的名字可以任意命名，常见的约定是使用origin作为远程克隆的仓库的别名 （这个别名会在运行git clone自动创建），upstream（上游）作为正式仓库的别名。 1git remote add upstream https://bitbucket.org/maintainer/repo 需要自己用上面的命令创建upstream别名。这样可以简单地保持本地仓库和正式仓库的同步更新。 注意，如果上游仓库需要认证（比如不是开源的），你需要提供用户： 1git remote add upstream https://user@bitbucket.org/maintainer/repo.git 这时在克隆和pull正式仓库时，需要提供用户的密码。 ¶开发者开发自己的功能 在刚克隆的本地仓库中，开发者可以像其它工作流一样的编辑代码、提交修改和新建分支： 123git checkout -b some-feature# Edit some codegit commit -a -m "Add first draft of some feature" 所有的修改都是私有的直到push到自己公开仓库中。如果正式项目已经往前走了，可以用git pull命令获得新的提交： 1git pull upstream master 由于开发者应该都在专门的功能分支上工作，pull操作结果会都是快进合并。 ¶开发者发布自己的功能 一旦开发者准备好了分享新功能，需要做二件事。 首先，通过push他的贡献代码到自己的公开仓库中，让其它的开发者都可以访问到。 他的origin远程别名应该已经有了，所以要做的就是： 1git push origin feature-branch 这里和之前的工作流的差异是，origin远程别名指向开发者自己的服务端仓库，而不是正式仓库。 第二件事，开发者要通知项目维护者，想要合并他的新功能到正式库中。 Bitbucket和Stash提供了Pull Request按钮，弹出表单让你指定哪个分支要合并到正式仓库。 一般你会想集成你的功能分支到上游远程仓库的master分支中。 ¶项目维护者集成开发者的功能 当项目维护者收到pull request，他要做的是决定是否集成它到正式代码库中。有二种方式来做： 直接在pull request中查看代码 pull代码到他自己的本地仓库，再手动合并 第一种做法更简单，维护者可以在GUI中查看变更的差异，做评注和执行合并。 但如果出现了合并冲突，需要第二种做法来解决。这种情况下，维护者需要从开发者的服务端仓库中fetch功能分支， 合并到他本地的master分支，解决冲突： 1234git fetch https://bitbucket.org/user/repo feature-branch# 查看变更git checkout mastergit merge FETCH_HEAD 变更集成到本地的master分支后，维护者要push变更到服务器上的正式仓库，这样其它的开发者都能访问到： 1git push origin master 注意，维护者的origin是指向他自己公开仓库的，即是项目的正式代码库。到此，开发者的贡献完全集成到了项目中。 ¶开发者和正式仓库做同步 由于正式代码库往前走了，其它的开发需要和正式仓库做同步： 1git pull upstream master 如果你之前是使用SVN，Forking工作流可能看起来像是一个激进的范式切换（paradigm shift）。 但不要害怕，这个工作流实际上就是在功能分支工作流之上引入另一个抽象层。 不是直接通过单个中央仓库来分享分支，而是把贡献代码发布到开发者自己的服务端仓库中。 示例中解释了，一个贡献如何从一个开发者流到正式的master分支中，但同样的方法可以把贡献集成到任一个仓库中。 比如，如果团队的几个人协作实现一个功能，可以在开发之间用相同的方法分享变更，完全不涉及正式仓库。 这使得Forking工作流对于松散组织的团队来说是个非常强大的工具。任一开发者可以方便地和另一开发者分享变更，任何分支都能有效地合并到正式代码库中。 ¶Pull Requests Pull requests是Bitbucket提供的让开发者更方便地进行协作的功能，提供了友好的Web界面可以在提议的修改合并到正式项目之前对修改进行讨论。 开发者向团队成员通知功能开发已经完成，Pull Requests是最简单的用法。 开发者完成功能开发后，通过Bitbucket账号发起一个Pull Request。 这样让涉及这个功能的所有人知道要去做Code Review和合并到master分支。 但是，Pull Request远不止一个简单的通知，而是为讨论提交的功能的一个专门论坛。 如果变更有任何问题，团队成员反馈在Pull Request中，甚至push新的提交微调功能。 所有的这些活动都直接跟踪在Pull Request中。 相比其它的协作模型，这种分享提交的形式有助于打造一个更流畅的工作流。 SVN和Git都能通过一个简单的脚本收到通知邮件；但是，讨论变更时，开发者通常只能去回复邮件。 这样做会变得杂乱，尤其还要涉及后面的几个提交时。 Pull Requests把所有相关功能整合到一个和Bitbucket仓库界面集成的用户友好Web界面中。 ¶解析 Pull Request 当要发起一个Pull Request，你所要做的就是请求（Request）另一个开发者（比如项目的维护者） 来pull你仓库中一个分支到他的仓库中。这意味着你要提供4个信息以发起Pull Request： 源仓库、源分支、目的仓库、目的分支。 这几值多数Bitbucket都会设置上合适的缺省值。但取决你用的协作工作流，你的团队可能会要指定不同的值。 上图显示了一个Pull Request请求合并一个功能分支到正式的master分支上，但可以有多种不同的Pull Request用法。 ¶工作方式 Pull Request可以和功能分支工作流、Gitflow工作流或Forking工作流一起使用。 但一个Pull Request要求要么分支不同要么仓库不同，所以不能用于集中式工作流。 在不同的工作流中使用Pull Request会有一些不同，但基本的过程是这样的： 开发者在本地仓库中新建一个专门的分支开发功能。 开发者push分支修改到公开的Bitbucket仓库中。 开发者通过Bitbucket发起一个Pull Request。 团队的其它成员review code，讨论并修改。 项目维护者合并功能到官方仓库中并关闭Pull Request。 本文后面内容说明，Pull Request在不同协作工作流中如何应用。 ¶在功能分支工作流中使用Pull Request 功能分支工作流用一个共享的Bitbucket仓库来管理协作，开发者在专门的分支上开发功能。 但不是立即合并到master分支上，而是在合并到主代码库之前开发者应该开一个Pull Request发起功能的讨论。 功能分支工作流只有一个公开的仓库，所以Pull Request的目的仓库和源仓库总是同一个。 通常开发者会指定他的功能分支作为源分支，master分支作为目的分支。 收到Pull Request后，项目维护者要决定如何做。如果功能没问题，就简单地合并到master分支，关闭Pull Request。 但如果提交的变更有问题，他可以在Pull Request中反馈。之后新加的提交也会评论之后接着显示出来。 在功能还没有完全开发完的时候，也可能发起一个Pull Request。 比如开发者在实现某个需求时碰到了麻烦，他可以发一个包含正在进行中工作的Pull Request。 其它的开发者可以在Pull Request提供建议，或者甚至直接添加提交来解决问题。 ¶在Gitflow工作流中使用Pull Request Gitflow工作流和功能分支工作流类似，但围绕项目发布定义一个严格的分支模型。 在Gitflow工作流中使用Pull Request让开发者在发布分支或是维护分支上工作时， 可以有个方便的地方对关于发布分支或是维护分支的问题进行交流。 Gitflow工作流中Pull Request的使用过程和上一节中完全一致： 当一个功能、发布或是热修复分支需要Review时，开发者简单发起一个Pull Request， 团队的其它成员会通过Bitbucket收到通知。 新功能一般合并到develop分支，而发布和热修复则要同时合并到develop分支和master分支上。 Pull Request可能用做所有合并的正式管理。 ¶在Forking工作流中使用Pull Request 在Forking工作流中，开发者push完成的功能到他自己的仓库中，而不是共享仓库。 然后，他发起一个Pull Request，让项目维护者知道他的功能已经可以Review了。 在这个工作流，Pull Request的通知功能非常有用， 因为项目维护者不可能知道其它开发者在他们自己的仓库添加了提交。 由于各个开发有自己的公开仓库，Pull Request的源仓库和目标仓库不是同一个。 源仓库是开发者的公开仓库，源分支是包含了修改的分支。 如果开发者要合并修改到正式代码库中，那么目标仓库是正式仓库，目标分支是master分支。 Pull Request也可以用于正式项目之外的其它开发者之间的协作。 比如，如果一个开发者和一个团队成员一起开发一个功能，他们可以发起一个Pull Request， 用团队成员的Bitbucket仓库作为目标，而不是正式项目的仓库。 然后使用相同的功能分支作为源和目标分支。 2个开发者之间可以在Pull Request中讨论和开发功能。 完成开发后，他们可以发起另一个Pull Request，请求合并功能到正式的master分支。 在Forking工作流中，这样的灵活性让Pull Request成为一个强有力的协作工具。 ¶示例 下面的示例演示了Pull Request如何在在Forking工作流中使用。 也同样适用于小团队的开发协作和第三方开发者向开源项目的贡献。 在示例中，小红是个开发，小明是项目维护者。他们各自有一个公开的Bitbucket仓库，而小明的仓库包含了正式工程。 ¶小红fork正式项目 小红先要fork小明的Bitbucket仓库，开始项目的开发。她登陆Bitbucket，浏览到小明的仓库页面， 点Fork按钮。 然后为fork出来的仓库填写名字和描述，这样小红就有了服务端的项目拷贝了。 ¶小红克隆她的Bitbucket仓库 下一步，小红克隆自己刚才fork出来的Bitbucket仓库，以在本机上准备出工作拷贝。命令如下： 1git clone https://user@bitbucket.org/user/repo.git 请记住，git clone会自动创建origin远程别名，是指向小红fork出来的仓库。 ¶小红开发新功能 在开始改代码前，小红要为新功能先新建一个新分支。她会用这个分支作为Pull Request的源分支。 123git checkout -b some-feature# 编辑代码git commit -a -m "Add first draft of some feature" 在新功能分支上，小红按需要添加提交。甚至如果小红觉得功能分支上的提交历史太乱了，她可以用交互式rebase来删除或压制提交。 对于大型项目，整理功能分支的历史可以让项目维护者更容易看出在Pull Request中做了什么内容。 ¶小红push功能到她的Bitbucket仓库中 小红完成了功能后，push功能到她自己的Bitbucket仓库中（不是正式仓库），用下面简单的命令： 1git push origin some-branch 这时她的变更可以让项目维护者看到了（或者任何想要看的协作者）。 ¶小红发起Pull Request Bitbucket上有了她的功能分支后，小红可以用她的Bitbucket账号浏览到她的fork出来的仓库页面， 点右上角的【Pull Request】按钮，发起一个Pull Request。 弹出的表单自动设置小红的仓库为源仓库，询问小红以指定源分支、目标仓库和目标分支。 小红想要合并功能到正式仓库，所以源分支是她的功能分支，目标仓库是小明的公开仓库， 而目标分支是master分支。另外，小红需要提供Pull Request的标题和描述信息。 如果需要小明以外的人审核批准代码，她可以把这些人填在【Reviewers】文本框中。 创建好了Pull Request，通知会通过Bitbucket系统消息或邮件（可选）发给小明。 ¶小明review Pull Request 在小明的Bitbucket仓库页面的【Pull Request】Tab可以看到所有人发起的Pull Request。 点击小红的Pull Request会显示出Pull Request的描述、功能的提交历史和每个变更的差异（diff）。 如果小明想要合并到项目中，只要点一下【Merge】按钮，就可以同意Pull Request并合并到master分支。 但如果像这个示例中一样小明发现了在小红的代码中的一个小Bug，要小红在合并前修复。 小明可以在整个Pull Request上加上评注，或是选择历史中的某个提交加上评注。 ¶小红补加提交 如果小红对反馈有任何疑问，可以在Pull Request中响应，把Pull Request当作是她功能讨论的论坛。 小红在她的功能分支新加提交以解决代码问题，并push到她的Bitbucket仓库中，就像前一轮中的做法一样。 这些提交会进入的Pull Request，小明在原来的评注旁边可以再次review变更。 ¶小明接受Pull Request 最终，小明接受变更，合并功能分支到Master分支，并关闭Pull Request。 至此，功能集成到项目中，其它的项目开发者可以用标准的git pull命令pull这些变更到自己的本地仓库中。 到了这里，你应该有了所有需要的工具来集成Pull Request到你自己的工作流。 请记住，Pull Request并不是为了替代任何 基于Git的协作工作流， 而是它们的一个便利的补充，让团队成员间的协作更轻松方便。 ¶企业日常开发模式探索 在看这部分前，请先回顾阅读业界认可的成功的 Git Branch Work Flow 模型 A Successful Git Branching Model ，了解日常开发中的场景，有助于熟悉下面的使用过程。 在企业开发中，使用 Git 作为版本控制软件最看重的还是结合公司自己搭建的 Gitlab，将 Code Review 加入打包部署持续集成的流程中，这样，代码开发完成，提交测试前，便可以对开发人员提交的代码进行 Review，发现潜在的问题，及时指导，对于新人来讲，也能更快更好的学习。 解决的需求场景如下： 能支持日常迭代开发、紧急线上bug修复、多功能并行开发 大概50人左右的团队，平日迭代项目较多，且周期短（1~2周一个迭代） 能够通过tag重建整个系统 支持code review 所有上线的代码必须都是经过测试保证，且能自动同步到下一次的迭代中 能和公司的项目管理/持续集成系统整合 上图就是 xirong 团队在日常开发中总结出来的适合企业开发的模式，下面进行简单的介绍，方便大家学习了解，欢迎提交 Issue 进行讨论。（本模式适合敏捷开发流程，小迭代上线，传统的瀑布开发模型并没有进行测试） 迭代需求会、冲刺会后确定本次迭代的目标后，将迭代内容视为一个项目，在 Gitlab 上创建一个 Repository，初始化工程代码结构，根据上线日期，比如20150730上线，开出分支 release20150730、dev20150730 两个分支，dev 分支作为日常开发主干分支，release 分支作为提测打包、Code Review 的分支。 迭代开始，日常开发进行中，开发人员在 dev 分支上进行 Commit、Push 代码，并且解决掉日常协同开发中的冲突等问题，等到达到提测条件的时候，提测者，首先 Merge Master 分支上的最新代码 git merge --no-ff origin/master ，使得 Master 分支上的变更更新到迭代开发分支dev上面，之后，在 Gitlab 上面发起 pull request 请求，并指定 Code Review 人，请求的分支选择本次上线的 release 分支，即 release20150730。 被指定 Code Review 的人，对发起者的代码 Review 后，决定是否可以提交测试，若有问题，评论注释代码后，提交者对代码进行进行修改，重复步骤2，直到代码 Review 者认为 Ok。之后便可以借助自己公司的打包部署，对这些代码发布到测试环境验证。 步骤2-3重复多次后，就会达到一个稳定可发布的版本，即上线版本，上线后，将 release 版本上面最后的提交（图中0.2.4上线对应处）合并到 Master 分支上面，并打 Tag0.3。至此，一次完整的迭代开发完成。 若此次上线后，不久发现生产环境有 Bug 需要修复，则从 Tag 处新开分支 release_bugfix_20150731、dev_bugfix_20150731 ，开发人员从 dev_bugfix_20150731分支上进行开发，提测code review在 release_bugfix_20150731 分支上，具体步骤参考2-3，测试环境验证通过后，发布到线上，验证OK，合并到 Master 分支，并打 Tag0.2.3，此次 Bug 修复完毕，专为解 Bug 而生的这两个分支可以退伍了，删除release_bugfix_20150731、dev_bugfix_20150731两分支即可。（所有的历史 Commit 信息均已经提交到了 Master 分支上，不用担心丢失） 这样经过上面的1-5步骤，企业日常迭代开发中的代码版本控制基本上就 Ok 了，有问题欢迎 Issue 讨论。 本文转载于 [@xirong](https://github.com/xirong)]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[邮件服务器 - sendmail/postfix]]></title>
    <url>%2F2017%2F10%2F31%2F2017%2F%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8-sendmail-postfix%2F</url>
    <content type="text"></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Security</tag>
        <tag>Fail2ban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让你的服务器更安全 - 使用 Fail2ban 自动检测恶意攻击]]></title>
    <url>%2F2017%2F10%2F31%2F2017%2F%E8%AE%A9%E4%BD%A0%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9B%B4%E5%AE%89%E5%85%A8-%E4%BD%BF%E7%94%A8-fail2ban-%E8%87%AA%E5%8A%A8%E6%A3%80%E6%B5%8B%E6%81%B6%E6%84%8F%E6%94%BB%E5%87%BB%2F</url>
    <content type="text"><![CDATA[对于 SSH 服务的常见的攻击就是暴力破解攻击——远程攻击者通过不同的密码来无限次地进行登录尝试。当然 SSH 可以设置使用非密码验证验证方式来对抗这种攻击，例如 公钥验证 或者 双重验证。将不同的验证方法的优劣处先放在一边，如果我们必须使用密码验证方式怎么办？你是如何保护你的 SSH 服务器免遭暴力破解攻击的呢？ fail2ban 是 Linux 上的一个著名的入侵保护的开源框架，它会监控多个系统的日志文件（例如：/var/log/auth.log 或者 /var/log/secure）并根据检测到的任何可疑的行为自动触发不同的防御动作。事实上，fail2ban在防御对 SSH 服务器的暴力密码破解上非常有用。 在这篇指导教程中，我会演示如何安装并配置 fail2ban 来保护 SSH 服务器以避免来自远程 IP 地址的暴力攻击。 注：本文仅介绍在 Ubuntu 中的使用方式，其他系统请参考官方手册。 ¶使用前提 你的服务器需要安装 Fail2ban，安装如下。 1$ sudo apt-get install fail2ban 你也许会和其他服务配置使用，如：iptables、sendmail，详情请参考： 让你的服务器更安全 - 使用 Iptables 防火墙 邮件服务器 - sendmail/postfix ¶如何使用 ¶基础定义 在使用之前，我们需要了解 fail2ban 中每个术语的意义与作用： filter：过滤器，定义了一系列正在表达式，用于匹配指定日志文件中的每行内容。 action：执行动作，当 fail2ban 检测到可疑行为时做出的动作。 jail：任务，定义 fail2ban 的自动检测任务，可以自由组合 filter 和 action。 fail2ban-server：fail2ban 服务器。 fail2ban-client：fail2ban 客户端。 ¶fail2ban 服务器 fail2ban 是一个多线程的服务器，用于执行 fail2ban-client 发送过来的命令。操作如下： 1$ sudo service fail2ban start|stop|restart ¶fail2ban 客户端 fail2ban 客户端，他通过 Unix domain socket 的方式连接服务器，通过发送命令去配置和操作 fail2ban 服务器，常用选项如下： 123456789-c &lt;DIR&gt; configuration directory-s &lt;FILE&gt; socket path-d dump configuration. For debugging-i interactive mode-v increase verbosity-q decrease verbosity-x force execution of the server-h, --help display this help message-V, --version print the version 配置文件里定义的所有配置都可以手动修改的，只需要通过 set 命令修改即可，如下： 12$ sudo fail2ban-client set loglevel 1$ sudo fail2ban-client set logtarget STDERR 启动服务： 1$ sudo fail2ban-client start 更新配置： 1$ sudo faile2ban-client reload ¶定义配置 首先，我们先来看一下 fail2ban 的配置目录结构： 123456789101112131415161718192021222324/etc/fail2ban/├── action.d│ ├── dummy.conf│ ├── hostsdeny.conf│ ├── iptables.conf│ ├── mail-whois.conf│ ├── mail.conf│ └── shorewall.conf├── fail2ban.conf├── fail2ban.local├── filter.d│ ├── apache-auth.conf│ ├── apache-noscript.conf│ ├── couriersmtp.conf│ ├── postfix.conf│ ├── proftpd.conf│ ├── qmail.conf│ ├── sasl.conf│ ├── sshd.conf│ └── vsftpd.conf├── jail.conf├── jail.local├── jail.d│ ├── your_jail.conf action.d：定义了一系列动作，如：发送邮件、使用 iptables 等，自定义的动作也放在该目录下。 fail2ban.conf：定义了 fail2ban 服务配置。 filter.d：定义了一系列过滤器，安装时会自带一些，自定义过滤器也房放在该目录下。 jail.conf：定义自动检测任务，包含一些默认参数，定义了一些常用服务的检测规则。 jail.d：自定义检测任务规则，存放在该目录下。 ¶任务配置 ¶参考链接 Fail2ban Official Manual]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Security</tag>
        <tag>Fail2ban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让你的服务器更安全 - 使用 Iptables 防火墙]]></title>
    <url>%2F2017%2F10%2F31%2F2017%2F%E8%AE%A9%E4%BD%A0%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9B%B4%E5%AE%89%E5%85%A8-%E4%BD%BF%E7%94%A8-Iptables-%E9%98%B2%E7%81%AB%E5%A2%99%2F</url>
    <content type="text"></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Firewall</tag>
        <tag>Iptables</tag>
        <tag>Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 成长之路（一）]]></title>
    <url>%2F2017%2F08%2F03%2F2017%2Flinux-%E6%88%90%E9%95%BF%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux 是非常流行的服务器操作系统，他的命令行非常优美，使用起来非常高效、快捷。让我们一起来享受这 窈窕淑女。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式 - 回溯]]></title>
    <url>%2F2017%2F06%2F03%2F2017%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E5%9B%9E%E6%BA%AF%2F</url>
    <content type="text"><![CDATA[在正则表达式实现中，回溯是匹配过程的基本组成部分，它是正则表达式如此好用和强大的根源。然而，回溯计算代价很高，如果设计失误，将导致失控。回溯是影响整体性能的唯一因素，理解它的工作原理，以及如何减小使用频率，可能是编写高效正则表达式的关键点。 ¶匹配原理 当一个正则表达式扫描目标字符串时，从左到右逐个扫描正则表达式的组成部分，在每个位置上测试能不能找到一个匹配。对于每一个量词和分支，都必须确定如何继续进行。如果是一个量词（如 *、+、?或者 {2,}），那么正则表达式必须确定何时尝试匹配更多的字符；如果遇到分支（通过|操作符），那么正则表达式必须从这些选项中选择一个进行尝试。 当正则表达式做出这样的决定时，如果有必要，它会记住另一个选项，以备返回后使用。如果所选方案匹配成功，正则表达式将继续扫描正则表达式模板，如果其余部分匹配也成功了，那么匹配就结束了。但是，如果所选择的方案未能发现相应匹配，或者后来的匹配也失败了，正则表达式将回溯到最后一个决策点，然后在剩余的选项中选择一个。继续这样，直到找到一个匹配，或者量词和分支选项的所有可能的排列组合都尝试失败后放弃这一过程，然后移动到此过程开始位置的下一个字符上，重复此过程。 例如，下面的代码演示了这一过程是如何通过回溯处理分支的。 1/h(ello|appy) hippo/.test("hello there, happy hippo"); 上面一行正则表达式用于匹配 hello hippo 或 happy hippo。测试一开始要查找一个 h，目标字符串的第一个字母恰好就是 h，立刻就找到了。接下来，子表达式（ello|appy）提供了两个处理选项。正则表达式选择最左边的选项（分支选择总是从左到右进行），检查 ello 是否匹配字符串的下一个字符，确实匹配，然后正则表达式又匹配了后面的空格。 然而，在接下来的匹配中正则表达式“走进了死胡同”，因为 hippo 中的 h 不能匹配字符串中的下一个字母 t。此时正则表达式还不能放弃，因为它还没有尝试过所有的选择，随后它回溯到最后一个检查点（在匹配了首字母h 之后的那个位置上）并尝试匹配第二个分支选项。但由于匹配没有成功，而且也没有更多的选项了，正则表达式认为从字符串的第一个字符开始匹配是不能成功的，因此它从第二个字符开始重新进行查找。正则表达式没有找到h，继续向后找，直到第 14 个字母才找到，它匹配 happy 的那个 h。随后正则表达式再次进入分支过程，这次 ello 未能匹配，但在回溯之后的第二次分支中，它匹配了整个字符串 happy hippo，匹配成功了。 再如，下面代码演示了带重复量词的回溯。 12var str = "&lt;p&gt;Para 1.&lt;/p&gt;" +"&lt;img src='smiley.jpg'&gt;" +"&lt;p&gt;Para 2.&lt;/p&gt;" +"&lt;div&gt;Div.&lt;/div&gt;"; /&lt;p&gt;.*&lt;\/p&gt;/i.test(str); 正则表达式先匹配了字符串开始的 3 个字母 &lt;p&gt;，然后是 .*。点号表示匹配除换行符以外的任意字符，星号这个 贪性量词(表示重复零次或多次)，匹配尽量多的次数。因为目标字符串中没有换行符，正则表达式将匹配剩下的全部字符串！不过由于正则表达式模板中还有更多内容需要匹配，所以正则表达式尝试匹配 &lt; 。由于在字符串末尾匹配不成功，因此每次回溯一个字符，继续尝试匹配&lt;，直到正则表达式回到 &lt;/div&gt; 标签的 &lt; 位置。接下来尝试匹配 \/（转义反斜杠），匹配成功，然后匹配 p，匹配不成功。正则表达式继续回溯，重复此过程，直到第二段末尾时终于匹配了 &lt;/p&gt;。匹配返回成功需要从第一段头部一直扫描到最后一个的末尾，这可能不是我们想要的结果。 将正则表达式中的 贪性量词 改为 惰性量词（详细请查看如何设置量词为惰性），以匹配单个段落。惰性量词的回溯工作以相反方式进行。当正则表达式 /&lt;p&gt;.*?&lt;\/p&gt;/ 推进到 .*? 时，首先尝试全部跳过，然后继续匹配 &lt;\/p&gt;。 这样做是因为 *? 匹配零次或多次，尽可能少重复，尽可能少意味着可以重复零次。但是，当随后的 &lt; 在字符串的这一点上匹配失败时，正则表达式回溯并尝试下一个最小的字符数：1 个。正则表达式继续像这样向前回溯到第一段的末尾，在那里量词后面的 &lt;\/p&gt; 得到完全匹配。 如果目标字符串只有一个段落，那么此正则表达式的 贪性匹配 和 惰性匹配 是等价的，但尝试匹配的过程不同。 当一个正则表达式占用浏览器几秒甚至更长时间时，问题原因很可能是回溯失控。为说明此问题，给出下面的正则表达式，它的目标是匹配整个 HTML 文件。此表达式被拆分成多行是为了适合页面显示。与其他正则表达式不同，JavaScript 在没有选项时可使点号匹配任意字符，包括换行符，所以此例中以 [\s\S] 匹配任意字符。 12/&lt;html&gt;[\s\S]*?&lt;head&gt;[\s\S]*?&lt;title&gt;[\s\S]*?&lt;\/title&gt;[\s\S]*?&lt;\/head&gt; [\s\S]*?&lt;body&gt;[\s\S]*?&lt;\/body&gt;[\s\S]*?&lt;\/html&gt;/ ¶总结 回溯会大大降低我们正则表达式的执行效率与时间，所以我们需要尽量减少回溯的次数，这需要我们更加仔细的考虑每一个正则表达式。 ¶参考连接 Backreferences 正则表达式中的悲观回溯 Runaway Regular Expressions: Catastrophic Backtracking Regex Tutorial - Possessive Quantifiers Regex Tutorial - Atomic Grouping]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式 - 贪性与惰性]]></title>
    <url>%2F2017%2F06%2F03%2F2017%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E8%B4%AA%E6%80%A7%E4%B8%8E%E6%83%B0%E6%80%A7%2F</url>
    <content type="text"><![CDATA[使用正则表达式中的 贪性、惰性、支配性 的量词可以控制表达式匹配过程，我们知道量词 ？、*、+ 的意义，可以指定相关模式出现的次数，默认的情况下我们使用的量词均是是贪婪量词，它的匹配过程是从整个字符串开始查看，如果不匹配就去掉最后一个，再看看是否匹配，如此循环一直到匹配或字符串空为止。不管是 贪性 还是 惰性 ，在正则表达式在匹配失败时都会触发 正则表达式回溯（详情查看正则表达式 - 回溯），从而影响其性能。 ¶量词（Quantifier） 首先，我们需要了解一下正则表达式的量词，正则表达式中有如下几种量词 ?、+、*、{n}、{n,m}、{n,}。 量词 匹配规则 例子 结果 ? 匹配出现 0 次或 1 次 /a?/.test(‘abcd’); true + 匹配出现 1 次或多次 /a+/.test(‘aaaaaaaa’); true * 匹配出现 0 次或 1 次或多次 /a*/.test(‘bbbbb’); true {n} 匹配刚好出现 n 次 /a{5}/.test(‘aaaab’); false {n,m} 匹配最少出现 n 次最多出现 m 次 a{1,3}/.test(‘aaabbb’); true {n,} 匹配最少出现 n 次 /a{3,}/.test(‘aabbb’); false ¶贪性匹配（greediness） 从字面意思我们就可以知道，所谓的&quot;贪婪&quot;的意思就是，如果符合要求就一直往后匹配，一直到无法匹配为止，这就是贪性匹配。默认情况下，?、+、*、{n}、{n,m}、{n,}都是贪婪的，也就是说，它会根据前导字符去匹配尽可能多的内容。 贪性匹配就是匹配尽可能 多 的内容。 假如我们有一个字符串 This is a &lt;EM&gt;first&lt;/EM&gt; test，我们使用 /&lt;.+&gt;/g 来匹配，我们期望匹配到两段文本，第一段为 &lt;EM&gt;，第二段匹配文本为 &lt;/EM&gt;，但是只得到了一段匹配文本：&lt;EM&gt;first&lt;/EM&gt;，很显然结果不尽人意，并不是我们想要的。 出现这种情况的罪魁祸首就是 + 是 贪性匹配（greedy） 的，正则表达式引擎执行 + 量词时尽可能的重复匹配前面的符号 . 回溯（即：正则表达式放弃上一次匹配，吐出一个匹配的字符给剩下的字符串），再一次使用下一个正则表达式符号来匹配剩下的字符串，如此反复 回溯 直到匹配成功或回溯到 0。通过下面的例子，可以更加直观的解释他的过程。 &lt;.+&gt; 匹配字符串 This is a &lt;EM&gt;first&lt;/EM&gt; test 的过程： 第一个符号为 &lt;，所以它将会匹配到字符串中的第一个 &lt; 字符。 第二个符号为 .，匹配任意字符（除了换行），使用 + 重复匹配，所以它将会不断重复匹配，直到他匹配到句尾，则匹配失败，.+ 匹配了 EM&gt;first&lt;/EM&gt; test。 到目前为止 &lt;.+ 匹配的字符串为：&lt;EM&gt;first&lt;/EM&gt; test。 正则表达式引擎开始对下一个正则表达式符号 &gt; 进行匹配，但字符串还未匹配的只剩下换行符了，于是匹配失败。 由于第三个符号 &gt; 匹配失败，正则表达式引擎回溯，回溯会使 .+ 减少（吐出）一个字符，回溯后的匹配状况是 .+ 匹配 EM&gt;first&lt;/EM&gt; tes。 正则表达式引擎使用符号 &gt; 匹配 t，结果依然不匹配。 如果重复 5、6 两步，直到 .+ 匹配 EM&gt;first&lt;/EM 时，正则表达式使用符号 &gt; 匹配 &gt; test，匹配成功。 正则表达式引擎报告 &lt;EM&gt;first&lt;/EM&gt; 就是匹配上的文本。 ¶可选性（optional） 我们都知道量词 ? 可以使前一个正则表达式字符是可选的，如：colou?r 可以匹配 colour 和 color。当然你也可以使用括号 () 将一些字符或规则标识为可选的，如：Nov(ember)? 将会匹配 Nov 和 November，当然你也可以使用 {0,1} 拉替代 ?，他们是等价的。 默认情况下 ? 也是 贪性匹配 的，即：他会优先尝试匹配该字符存在的情况，如果匹配失败，则尝试匹配该字符不存在的情况。例如： 我们用 Feb 23(rd)? 表达式匹配 Today is Feb 23rd, 2003 文本，匹配结果是 Feb 23rd 而不是 Feb 23。 如果是：Feb 23(rd)?? 呢？结果却是 Feb 23，是不是很奇妙，下面我们将会详细解答惰性匹配的原理。 ¶惰性匹配（laziness） 惰性模式就是一旦匹配到合适的就结束，不在继续匹配下去了，在重复量词后面添加问号 ? 即可形成惰性匹配。惰性匹配会尽可能少的匹配字符，但是必须要满足整个匹配模式。 惰性匹配就是匹配尽可能 少 的内容。 量词 匹配规则 ?? 匹配出现 0 次或 1 次，优先匹配 0 次 +? 匹配出现 1 次或多次，尽可能重复少的次数，不过最少次数是 1 *? 匹配出现 0 次或 1 次或多次，尽可能重复少的次数 {n}? 匹配刚好出现 n 次 {n,m}? 匹配最少出现 n 次最多出现 m 次，尽可能重复少的次数，不过最少次数是 n {n,}? 匹配最少出现 n 次，尽可能重复少的次数，不过最少次数是 n &lt;.+?&gt; 匹配字符串 This is a &lt;EM&gt;first&lt;/EM&gt; test 的过程： 第一个符号为 &lt;，所以它将会匹配到字符串中的第一个 &lt; 字符。 第二个符号为 .，匹配任意字符（除了换行），使用 +? 重复匹配，因为是惰性匹配，所以 + 会尽可能少的匹配（最少一次），第一次 .+ 匹配了 E。 正则表达式引擎开始对下一个正则表达式符号 &gt; 进行匹配，剩下的字符串 M&gt;first&lt;/EM&gt; test 的第一个字符为 M，于是匹配失败。 由于第三个符号 &gt; 匹配失败，正则表达式引擎回溯，回溯会使 .+ 扩大（吞入）一个字符，而不是贪性匹配的减少一个字符，回溯后的匹配状况是 .+ 匹配 EM。 正则表达式引擎开始对下一个正则表达式符号 &gt; 进行匹配，剩下的字符串 &gt;first&lt;/EM&gt; test 的第一个字符为 &gt;，于是匹配成功。 正则表达式引擎报告 &lt;EM&gt; 就是匹配上的文本。 ¶总结 贪性匹配，正则引擎会一直匹配到字符串最后；当匹配为 false 时，就回溯以找到倒数第一个匹配位置，返回匹配结果。 惰性匹配，正则引擎会匹配到符合 pattern 的末尾位置那个字符，然后再往后走一步，发现匹配为 false 时，就回溯以找到最近一个匹配为 true 的位置，返回匹配结果。 改善 贪性、惰性，减少回溯次数，能够有利于正则表达式的执行效率，写出更好的正则表达式。 如我们可以使用 &lt;[^&gt;]+&gt; 来取代 &lt;.+?&gt; 和 &lt;.+&gt;，这样回溯次数就降到了 0 次，当然，效率和耗时都是最佳的。 ¶参考连接 Optional Items Repetition with Star and Plus]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 线程安全与非线程安全]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2Fjava-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E4%B8%8E%E9%9D%9E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[还记得初学 Java 的时候，老师是这样告诉我们的：ArrayList 是非线程安全的，Vector 是线程安全的；HashMap 是非线程安全的，HashTable 是线程安全的；StringBuilder 是非线程安全的，StringBuffer 是线程安全的。对，很快我们也记住了这一面试宝典，但是当面试官继续往下问你：什么是线程安全和非线程安全？他们有什么区别？什么场景用哪一种模式？这时，你是不是会吐上一口老血。 ¶原子性 首先，我们吸纳来了解一下什么是原子性，这个概念大家或许耳熟能详。 在多线程访问共享资源（同一块内存），能保证在同一时间，只有一个进程访问该资源，并且该操作不可被分割，不能被打断，必须执行到结束，不会被线程切换打断，这就是原子性。 这个在 Java 多线程编程中是老生常谈的话题了，所谓的原子操作是指不会被线程调度机制打断的操作，这种操作从执行开始就会一直执行结束，中间不会出现任何的 context switch（线程的切换）。 ¶线程安全与非线程安全 那到底什么是线程安全呢，其实线程安全就是一个原子操作，大致是：当一个线程调用一个线程安全的方法时，其他的线程就不能调用该线程安全的方法了，而必须要等到正在执行的线程结束后才能调用。 其实也是线程同步的问题，若是每个线程对共享内存都做只读操作，而无写操作，那么这块共享内存是线程安全，若是有多个线程会同时进行写操作，这时候就需要考虑线程同步的问题了，这就涉及到了线程安全之间问题。 ¶线程安全模拟 首先，我们用 ArrayList 来演示非线程安全，我们在主线程有一个 ArrayList，在主线程开启四个子线程分别往这个列表中插入值，]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎 ElasticSearch - 使用 Docker 搭建 ELK 系统]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-elasticsearch-%E4%BD%BF%E7%94%A8-docker-%E6%90%AD%E5%BB%BA-elk-%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"></content>
      <categories>
        <category>ElasticSearch</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
        <tag>ElasticSearch</tag>
        <tag>ELK</tag>
        <tag>LogStash</tag>
        <tag>Kibana</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎 ElasticSearch - 使用 IK 中文分词插件]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-elasticsearch-%E4%BD%BF%E7%94%A8-ik-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>ElasticSearch</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
        <tag>ElasticSearch</tag>
        <tag>IK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎 ElasticSearch - 构建自己的 HanLP 中文分词插件]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-elasticsearch-%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84-hanlp-%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
        <tag>ElasticSearch</tag>
        <tag>HanLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎 ElasticSearch - 搜索建议的实现]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-elasticsearch-%E6%90%9C%E7%B4%A2%E5%BB%BA%E8%AE%AE%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
        <tag>ElasticSearch</tag>
        <tag>搜索建议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎 ElasticSearch - 同义词方案]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-elasticsearch-%E5%90%8C%E4%B9%89%E8%AF%8D%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
        <tag>ElasticSearch</tag>
        <tag>同义词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索引擎 ElasticSearch - 如何搭配使用 IK 和 HanLP]]></title>
    <url>%2F2017%2F05%2F27%2F2017%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-elasticsearch-%E5%A6%82%E4%BD%95%E6%90%AD%E9%85%8D%E4%BD%BF%E7%94%A8-ik-%E5%92%8C-hanlp%2F</url>
    <content type="text"></content>
      <categories>
        <category>ElasticSearch</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>搜索引擎</tag>
        <tag>ElasticSearch</tag>
        <tag>IK</tag>
        <tag>HanLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让你的服务器更安全 - 初始化服务器配置]]></title>
    <url>%2F2017%2F05%2F20%2F2017%2F%E8%AE%A9%E4%BD%A0%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9B%B4%E5%AE%89%E5%85%A8-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[当你创建或购买了了一个新的的服务器的时候，有一些必要的步骤作来配置你的服务器，比如：root 用户权限过大问题、SSH 安全连接问题等等、使用虚拟内存等等。这将增加您的服务器的安全性和可用性，并将为后续行动提供坚实的基础。 下面我将会解释如何在 Ubuntu 14.04 中使用做一些安全性基本配置。 ¶创建用户 ¶使用 Root 用户登录 ¶创建一个新用户 ¶赋予 Root 权限 ¶禁用 Root 用户 ¶添加公钥认证 ¶配置时区和 NTP ¶创建虚拟内存 ¶配置时区和 NTP ¶配置防火墙 配置防火墙是提高服务器安全方面很重要的一部分，他可以有限提高你的服务器的安全性和可用性，具体配置可以参考 让你的服务器更安全 - 使用 UFW 开启防火墙]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让你的服务器更安全 - 使用 UFW 开启防火墙]]></title>
    <url>%2F2017%2F05%2F20%2F2017%2F%E8%AE%A9%E4%BD%A0%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9B%B4%E5%AE%89%E5%85%A8-%E4%BD%BF%E7%94%A8-UFW-%E5%BC%80%E5%90%AF%E9%98%B2%E7%81%AB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[UFW(Uncomplicated Firewall) 是一个非常容易上手的 iptables 类防火墙配置工具，这个工具可以对出入服务的网络数据进行分割、过滤、转发等等细微的控制，进而实现诸如防火墙、 NAT 等功能。它简化了 iptable 那复杂的配置过程。我们都知道 iptable 非常强大、灵活，但是对于初学者来学习如何使用它正确的配置防火墙是比较难的，但是你又想保护你的网络，UFW 将会是你最好的选择。 下面我将会解释如何在 Ubuntu 14.04 中使用 UFW 安装、配置防火墙。 ¶使用前提 在你使用这片教材之前，我希望你有一个独立的 no-root 超级管理员用户 - 拥有 root 的所有权限。你可以查看我这篇文章 让你的服务器更安全 - 初始化服务器配置 中创建用户相关步骤。 一般来说 UFW 是默认会被安装的，假如你的系统中没有安装，你可以使用 apt-get 来安装。 1$ sudo apt-get install ufw ¶使用 IPv6 如果你的 Ubuntu 服务器已启用 IPv6，为了确保 UFW 能支持 IPv6 协议。 打开 UFW 的相关配置，使用你最喜欢的编辑器，这里我使用 vim ： 1$ vim /etc/default/ufw 然后，确认 IPv6 是否设置成 yes，如果没有则设置为 yes，大致如下： 123...IPV6=yes... 退出并保存，当 UFW 开启时，它将会同时支持 IPv4 和 IPv6 的配置规则。 ¶查看 UFW 状态和配置规则 在任何时间，你都可以检查它的状态和配置规则，如下： 1$ sudo ufw status verbose 默认情况下，UFW 并没有开启，它将会输出如下结果： 12# Output:Status: inactive 假如你已经开启了防火墙，它将会输出状态为 active，并列出你所配置的规则。例如：你允许来自任何地方的 SSH 连接，将将会输出如下结果： 123456789# OutputStatus: activeLogging: on (low)Default: deny (incoming), allow (outgoing), disabled (routed)New profiles: skipTo Action From-- ------ ----22/tcp ALLOW IN Anywhere 像这样通过 status 就可以检查你的防火墙状态和配置了。 注意：在开启防火墙之前，你需要确保你允许了 SSH 连接，否则当你关闭远程连接后，你就无法再连上了。博主自己就曾用这招坑了自己！ ¶设置默认规则 当你需要开始配置你的防火墙规则时，首先，你需要设置默认规则：拒绝所有流入连接，允许流出连接。意思是，不允许任何人连接你的主机，允许主机内的任何应用访问外部网络。 12$ sudo ufw default deny incoming$ sudo ufw default allow outgoing ¶开启 SSH 连接 上面我们已经设置了默认不接受任何外来连接，同样也包括了 SSH 使用的 22 端口。所以，为了我们能通过 SSH 来操作主机，所以我们需要配置允许 SSH 连接到我们的主机上。 通过如下命令来配置： 1$ sudo ufw allow ssh 这个配置将会允许所有 22 端口上的连接，默认 22 端口是被 SSH 监听的。UFW 知道什么是 ssh，因为它在 /etc/services 中已经被定义好了。 当然我们也可以指定允许 22 端口的所有连接： 1$ sudo ufw allow 22 这个和上面一个命令的作用是一样的。 ¶开启 UFW 上面已经允许 SSH 连接，我们就可以放心的开启防火墙了，使用如下命令： 1$ sudo ufw enable 在这个过程中，你将会收到一条警告信息（command may disrupt existing ssh connections.），需要你手动确认，输入 y 按回车即可。 太棒了，我们已经开启了防火墙，你可以再一次通过 sudo ufw status verbose 来查看。 ¶配置其他规则 ¶HTTP/HTTPS 当我们部署 WEB 引用服务器时，我们需要使用 80 或 443 端口来接受请求，这是我们需要开启这两个端口，操作如下： 12$ sudo ufw allow http$ sudo ufw allow https 或者，你可以指定端口： 12$ sudo ufw allow 80$ sudo ufw allow 443 ¶FTP FTP 连接一般用于非加密文件传输，它默认监听 21 端口，也许你永远都不会用到。 1$ sudo ufw allow ftp 或者，你可以指定端口： 1$ sudo ufw allow 21/tcp ¶指定端口范围 你可以指定一个端口范围，来配置防火墙策略，当有些服务需要使用多个端口时，这个就起到了作用。 如，为了允许所有 X11 连接，他们使用的端口范围是 6000 ~ 6007，你可以这样配置： 12$ sudo ufw allow 6000:6007/tcp$ sudo ufw allow 6000:6007/udp 指定端口范围时，你必须指定协议类型（TCP 或 UDP）。 ¶指定 IP 地址 使用 UFW 工作的时候，你可以指定 IP 地址，例如：假如你想允许来自某一个 IP 所有连接，你可以指定 from 这个 IP 地址。 1$ sudo ufw allow from 192.168.66.213 上面的配置将会允许 192.168.66.213 连接到我们主机的任何开放了的端口。 我们还可以指定只允许某个 IP 到主机某一个端口的连接，拒绝某个 IP 到主机其他所有端口的连接，我们可以这样做： 1$ sudo ufw allow from 192.168.66.213 to any port 80 上面配置中，我们只允许 192.168.66.213 连接到我们的 80 端口。 ¶配置子网 当你需要允许子网内所有的 IP，你可以 CIDR 的格式来配置，例如：当你需要允许 IP 地址从 192.168.1.1 到 192.168.1.254 内所有 IP 的连接时，你可以这样配置： 1$ sudo ufw allow from 192.168.1.1/24 当然，像上面一样，我们也可以同时指定端口号： 1$ sudo ufw allow from 192.168.1.1/24 to any port 22 上面配置中，我们允许 192.168.1.1/24 内的所有主机通过 SSH 连接我们的主机。 ¶指定网络接口 如果您想创建只适用于特定网络接口的防火墙规则，您可以通过指定 allow in on 加上网络接口的名称 来配置规则。 在配置之前你可以先查找所有的网络接口，再配置： 1$ ip addr 123456# Output Excerpt:...2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state...3: eth1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default ... 上面列举了网络接口的一些信息，他们通常叫做 eth0 或 eth1 之类的名字。 假如你的 eth0 为公网地址，你同事需要向外开放 80 端口，你可以如下操作： 1$ sudo ufw allow in on eth0 to any port 80 上面配置中，你的服务器将会接受来自于公网的 HTTP 请求。 另外，假如你想你的 MySQL 服务器（监听 3306）只接受通过内网网卡 eth1 的请求，你可以这样： 1$ sudo ufw allow in on eth1 to any port 3306 如上配置中，只有在用一个内网中的服务器才能连接你的 MySQL 服务器。 ¶添加拒绝连接规则 假如你没有修改过我们上面设置过的默认规则，它将会拒绝所有的外来连接，通常情况下，这样大大的简化了你配置一系列的防火墙规则，比如要求你创建指定端口啊，指定 IP 啊等等。但是，如果你想拒绝某个 IP 源或者某个网段的特定连接；也许你知道攻击源就来自于某个 IP 或某个网段；再者，你想把默认外接规则(incomming rule) 设置为 allow，这是你就需要指定某些拒绝规则了。 配置 拒绝规则 ，更我们上面配置 允许规则 是一样的方式，只不过将 allow 改为 deny。 如：拒绝所有 HTTP 连接，即：拒绝所有连接 80 端口。 1sudo ufw deny http 当然，也可以指定端口号： 1sudo ufw deny 80 拒绝某一个 192.168.1.10 通过 SSH 连接到我们的主机上： 1$ sudo ufw deny from 192.168.1.10 to any port 22 如果你想书写更多的 拒绝规则 ，请参考之前描述的 允许规则 书写方式，将 allow 改为 deny 即可。 现在我们知道如何添加 允许规则 和 拒绝规则，但是我们还不知道如何删除规则，没关系，我们再往下看。 ¶删除规则 众所周知，如何删除一条防火墙规则和如何创建一条防火墙规则一样重要，UFW 提供了两种路径删除他们： 通过规则序号来删除 通过实际规则来删除 ¶指定规则序号删除 每一个规则在创建时都会分配一个序号，你可以将它理解为数据库的自增 ID 吧，可以通过他来进行更方便的操作，你可以通过如下方式查看序号： 1sudo ufw status numbered 1234567Numbered Output:Status: active To Action From -- ------ ----[ 1] 22 ALLOW IN 15.15.15.0/24[ 2] 80 ALLOW IN Anywhere 假如你想删除第 2 条规则，拒绝所有指向 80 端口的连接，如下操作： 1$ sudo ufw delete 2 注意：如果你启用了 IPv6 规则，这同时也将删除相应的 IPv6 规则。 ¶指定实际规则删除 如果你不想通过 规则序号 来删除，你可以指定 创建时的参数格式 来删除，例如：当你使用 sudo ufw allow http 创建的规则时，你可以通过如下方式删除： 1$ sudo ufw delete allow http 同样你可以指定端口号来替代服务名： 1$ sudo ufw delete allow 80 注意：这种方式将会同时删除相应的 IPv4 和 IPv6规则。 ¶关闭 UFW 现在，由于某些我们不想开启防火墙了，我们可以关闭它： 1$ sudo ufw disable ¶重置 UFW 的配置 将入你配置了好多规则，但是你现在需要重新配置，抛弃之前的配置规则，你可以这么做： 1$ sudo ufw reset 这个命令将会删除你之前配置的所有规则，但是默认规则将会被保留。 ¶总结 服务器安全一直都是一个重要的话题，开启防火墙使我们保护服务器安全的重要手段之一，所以，无论什么情况下，我们都应该为服务器开启防火墙。当然，开放 SSH 也是必不可少的，与此同时，你可以允许一些连接到您的服务器，同时并限制一些不必要的连接，这样您的服务器才会更加的安全的提供服务。 想了解更多的 UFW 防火墙配置，你可以参考这篇文章：UFW Essentials: Common Firewall Rules and Commands]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Firewall</tag>
        <tag>Security</tag>
        <tag>UFW</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 基础之命令详解]]></title>
    <url>%2F2017%2F05%2F20%2F2017%2FRedis-%E5%9F%BA%E7%A1%80%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
</search>